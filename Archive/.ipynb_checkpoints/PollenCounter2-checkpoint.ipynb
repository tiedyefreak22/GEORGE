{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# PollenCounter\n",
    "Gradient-Effected Object Recognition Gauge for hive Entrances (GEORGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPs64QA1Zdov"
   },
   "source": [
    "## Imports (Colab, TF3, C3, W2, Lab 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uZcqD4NLdnf4",
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "import glob\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "#from object_detection.utils import colab_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IogyryF2lFBL"
   },
   "source": [
    "## Utilities (Colab, TF3, C3, W2, Lab 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-y9R0Xllefec",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "  Puts image into numpy array to feed into tensorflow graph.\n",
    "  Note that by convention we put it into a numpy array with shape\n",
    "  (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "  Args:\n",
    "    path: a file path.\n",
    "\n",
    "  Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "  image = Image.open(BytesIO(img_data))\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def plot_detections(image_np,\n",
    "                    boxes,\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    category_index,\n",
    "                    figsize=(12, 16),\n",
    "                    image_name=None):\n",
    "  \"\"\"Wrapper function to visualize detections.\n",
    "\n",
    "  Args:\n",
    "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    boxes: a numpy array of shape [N, 4]\n",
    "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
    "      and match the keys in the label map.\n",
    "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
    "      this function assumes that the boxes to be plotted are groundtruth\n",
    "      boxes and plot all boxes as black with no classes or scores.\n",
    "    category_index: a dict containing category dictionaries (each holding\n",
    "      category index `id` and category name `name`) keyed by category indices.\n",
    "    figsize: size for the figure.\n",
    "    image_name: a name for the image file.\n",
    "  \"\"\"\n",
    "  image_np_with_annotations = image_np.copy()\n",
    "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_annotations,\n",
    "      boxes,\n",
    "      classes,\n",
    "      scores,\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      min_score_thresh=0.8)\n",
    "  if image_name:\n",
    "    plt.imsave(image_name, image_np_with_annotations)\n",
    "  else:\n",
    "    plt.imshow(image_np_with_annotations)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSaXL28TZfk1"
   },
   "source": [
    "## BeeDataset Utilities (BeeDataset)\n",
    "\n",
    "The following cell contains class extensions and functions to load bee images and metadata.  The dataset from https://www.tensorflow.org/datasets/catalog/bee_dataset consists of 7,507 images and a set of labels that expose certain characterisitics of that images, such as varroa-mite infections, bees carrying pollen-packets or bee that are cooling the hive by flapping their wings. Additionally, this dataset contains images of wasps to be able to distinguish bees and wasps.\n",
    "\n",
    "The images of the bees are taken from above and rotated. The bee is vertical and either its head or the trunk is on top. All images were taken with a green background and the distance to the bees was always the same, thus all bees have the same size.\n",
    "\n",
    "Each image can have multiple labels assigned to it. E.g. a bee can be cooling the hive and have a varroa mite infection at the same time.\n",
    "\n",
    "This dataset is designed as multi-label dataset, where each label, e.g. varroa_output, contains 1 if the characterisitic was present in the image and a 0 if it wasn't. All images are provided by 300 pixel height and 150 pixel witdh. As default the dataset provides the images as 150x75 (h,w) pixel. You can select 300 pixel height by loading the datset with the name \"bee_dataset/bee_dataset_300\" and with 200 pixel height by \"bee_dataset/bee_dataset_200\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"BeeDataset dataset.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow_datasets.public_api as tfds\n",
    "\n",
    "\n",
    "class BeeDatasetConfig(tfds.core.BuilderConfig):\n",
    "  \"\"\"BuilderConfig for the BeeDataset.\n",
    "\n",
    "  Args:\n",
    "  image_width (int): Desired image width.\n",
    "  image_height (int): Desired image heigth.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, image_height=300, image_width=150, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.width = image_width\n",
    "    self.height = image_height\n",
    "    self.depth = 3\n",
    "\n",
    "\n",
    "class SetBuilder(tfds.core.GeneratorBasedBuilder):\n",
    "  \"\"\"DatasetBuilder for BeeDataset dataset.\"\"\"\n",
    "\n",
    "  VERSION = tfds.core.Version('1.0.0')\n",
    "\n",
    "  URL = 'https://raspbee.de/BeeDataset_20201121.zip'\n",
    "\n",
    "  BEE_CFG_300 = BeeDatasetConfig(\n",
    "      name='bee_dataset_300',\n",
    "      description='BeeDataset images with 300 pixel height and 150 pixel width',\n",
    "      version='1.0.0',\n",
    "      image_height=300,\n",
    "      image_width=150,\n",
    "  )\n",
    "\n",
    "  BEE_CFG_200 = BeeDatasetConfig(\n",
    "      name='bee_dataset_200',\n",
    "      description='BeeDataset images with 200 pixel height and 100 pixel width',\n",
    "      version='1.0.0',\n",
    "      image_height=200,\n",
    "      image_width=100,\n",
    "  )\n",
    "\n",
    "  BEE_CFG_150 = BeeDatasetConfig(\n",
    "      name='bee_dataset_150',\n",
    "      description='BeeDataset images with 200 pixel height and 100 pixel width',\n",
    "      version='1.0.0',\n",
    "      image_height=150,\n",
    "      image_width=75,\n",
    "  )\n",
    "\n",
    "  BUILDER_CONFIGS = [BEE_CFG_300, BEE_CFG_200, BEE_CFG_150]\n",
    "\n",
    "  def _info(self) -> tfds.core.DatasetInfo:\n",
    "    \"\"\"Returns the dataset metadata.\"\"\"\n",
    "    t_shape = (\n",
    "        self.builder_config.height,\n",
    "        self.builder_config.width,\n",
    "        self.builder_config.depth,\n",
    "    )\n",
    "    features = tfds.features.FeaturesDict({\n",
    "        'input': tfds.features.Image(shape=t_shape),\n",
    "        'output': {\n",
    "            'varroa_output': np.float64,\n",
    "            'pollen_output': np.float64,\n",
    "            'wasps_output': np.float64,\n",
    "            'cooling_output': np.float64,\n",
    "        },\n",
    "    })\n",
    "\n",
    "    return self.dataset_info_from_configs(\n",
    "        features=features,\n",
    "        supervised_keys=('input', 'output'),\n",
    "        homepage='https://raspbee.de',\n",
    "    )\n",
    "\n",
    "  def _split_generators(self, dl_manager):\n",
    "    \"\"\"Returns SplitGenerators.\"\"\"\n",
    "    path = dl_manager.download_and_extract(self.URL)\n",
    "    return {\n",
    "        'train': self._generate_examples(path),\n",
    "    }\n",
    "\n",
    "  def _generate_examples(self, path):\n",
    "    # Load labels and image path.\n",
    "    data = json.loads((path / 'data.json').read_text())\n",
    "    indexes = list(data.keys())\n",
    "    random.shuffle(indexes)\n",
    "    for name in indexes:\n",
    "      labels = []\n",
    "      entry = data[name]\n",
    "\n",
    "      for lbl in ['varroa', 'pollen', 'wasps', 'cooling']:\n",
    "        labels.append(1.0 if entry[lbl] else 0.0)\n",
    "\n",
    "      img = path / f'images_{self.builder_config.height}' / name\n",
    "\n",
    "      yield name + str(self.builder_config.height), {\n",
    "          'input': img,\n",
    "          'output': {\n",
    "              'varroa_output': labels[0],\n",
    "              'pollen_output': labels[1],\n",
    "              'wasps_output': labels[2],\n",
    "              'cooling_output': labels[3],\n",
    "          },\n",
    "      }\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsjDCIat6_UK"
   },
   "source": [
    "## Image Classification and Object Localization (Colab, TF3, C3, W1, Lab 3)\n",
    "\n",
    "- Place each \"bee\" image on a canvas of width (TBD) at random locations.\n",
    "- Calculate the corresponding bounding boxes for those \"bees\".\n",
    "\n",
    "The bounding box prediction can be modelled as a \"regression\" task, which means that the model will predict a numeric value (as opposed to a category)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpiJj8ym0v0-"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AoilhmYe1b5t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.13.0\n"
     ]
    }
   ],
   "source": [
    "import os, re, time, json\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmoFKEd98MP3"
   },
   "source": [
    "## Visualization Utilities\n",
    "\n",
    "These functions are used to draw bounding boxes around the bees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tBjj1Fg-i_lc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#@title Plot Utilities for Bounding Boxes [RUN ME]\n",
    "\n",
    "im_width = 75\n",
    "im_height = 150\n",
    "use_normalized_coordinates = True\n",
    "\n",
    "def draw_bounding_boxes_on_image_array(image,\n",
    "                                       boxes,\n",
    "                                       color=[],\n",
    "                                       thickness=1,\n",
    "                                       display_str_list=()):\n",
    "  \"\"\"Draws bounding boxes on image (numpy array).\n",
    "  Args:\n",
    "    image: a numpy array object.\n",
    "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "           The coordinates are in normalized format between [0, 1].\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list_list: a list of strings for each bounding box.\n",
    "  Raises:\n",
    "    ValueError: if boxes is not a [N, 4] array\n",
    "  \"\"\"\n",
    "  image_pil = PIL.Image.fromarray(image)\n",
    "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
    "  rgbimg.paste(image_pil)\n",
    "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness,\n",
    "                               display_str_list)\n",
    "  return np.array(rgbimg)\n",
    "  \n",
    "\n",
    "def draw_bounding_boxes_on_image(image,\n",
    "                                 boxes,\n",
    "                                 color=[],\n",
    "                                 thickness=1,\n",
    "                                 display_str_list=()):\n",
    "  \"\"\"Draws bounding boxes on image.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "           The coordinates are in normalized format between [0, 1].\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: a list of strings for each bounding box.\n",
    "                           \n",
    "  Raises:\n",
    "    ValueError: if boxes is not a [N, 4] array\n",
    "  \"\"\"\n",
    "  boxes_shape = boxes.shape\n",
    "  if not boxes_shape:\n",
    "    return\n",
    "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
    "    raise ValueError('Input must be of size [N, 4]')\n",
    "  for i in range(boxes_shape[0]):\n",
    "    draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3],\n",
    "                               boxes[i, 2], color[i], thickness, display_str_list[i])\n",
    "        \n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color='red',\n",
    "                               thickness=1,\n",
    "                               display_str=None,\n",
    "                               use_normalized_coordinates=True):\n",
    "  \"\"\"Adds a bounding box to an image.\n",
    "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    ymin: ymin of bounding box.\n",
    "    xmin: xmin of bounding box.\n",
    "    ymax: ymax of bounding box.\n",
    "    xmax: xmax of bounding box.\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: string to display in box\n",
    "    use_normalized_coordinates: If True (default), treat coordinates\n",
    "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
    "      coordinates as absolute.\n",
    "  \"\"\"\n",
    "  draw = PIL.ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "             (right, top), (left, top)], width=thickness, fill=color)\n",
    "print(\"Done\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USx9tRBF8hWy"
   },
   "source": [
    "These utilities are used to visualize the data and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qhdz68Xm3Z4Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#@title Visualization Utilities [RUN ME]\n",
    "\"\"\"\n",
    "This cell contains helper functions used for visualization\n",
    "and downloads only. \n",
    "\n",
    "You can skip reading it, as there is very\n",
    "little Keras or Tensorflow related code here.\n",
    "\"\"\"\n",
    "\n",
    "# Matplotlib config\n",
    "plt.rc('image', cmap='gray')\n",
    "plt.rc('grid', linewidth=0)\n",
    "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
    "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
    "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
    "plt.rc('text', color='a8151a')\n",
    "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
    "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
    "\n",
    "# pull a batch from the datasets. This code is not very nice, it gets much better in eager mode (TODO)\n",
    "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
    "  \n",
    "  # get one batch from each: 10000 validation digits, N training digits\n",
    "  batch_train_ds = training_dataset.unbatch().batch(N)\n",
    "  \n",
    "  # eager execution: loop through datasets normally\n",
    "  if tf.executing_eagerly():\n",
    "    for validation_digits, (validation_labels, validation_bboxes) in validation_dataset:\n",
    "      validation_digits = validation_digits.numpy()\n",
    "      validation_labels = validation_labels.numpy()\n",
    "      validation_bboxes = validation_bboxes.numpy()\n",
    "      break\n",
    "    for training_digits, (training_labels, training_bboxes) in batch_train_ds:\n",
    "      training_digits = training_digits.numpy()\n",
    "      training_labels = training_labels.numpy()\n",
    "      training_bboxes = training_bboxes.numpy()\n",
    "      break\n",
    "  \n",
    "  # these were one-hot encoded in the dataset\n",
    "  validation_labels = np.argmax(validation_labels, axis=1)\n",
    "  training_labels = np.argmax(training_labels, axis=1)\n",
    "  \n",
    "  return (training_digits, training_labels, training_bboxes,\n",
    "          validation_digits, validation_labels, validation_bboxes)\n",
    "\n",
    "# create digits from local fonts for testing\n",
    "def create_digits_from_local_fonts(n):\n",
    "  font_labels = []\n",
    "  img = PIL.Image.new('LA', (75*n, 75), color = (0,255)) # format 'LA': black in channel 0, alpha in channel 1\n",
    "  font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\n",
    "  font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\n",
    "  d = PIL.ImageDraw.Draw(img)\n",
    "  for i in range(n):\n",
    "    font_labels.append(i%10)\n",
    "    d.text((7+i*75,0 if i<10 else -4), str(i%10), fill=(255,255), font=font1 if i<10 else font2)\n",
    "  font_digits = np.array(img.getdata(), np.float32)[:,0] / 255.0 # black in channel 0, alpha in channel 1 (discarded)\n",
    "  font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [75, 75*n]), n, axis=1), axis=0), [n, 75*75])\n",
    "  return font_digits, font_labels\n",
    "\n",
    "\n",
    "# utility to display a row of digits with their predictions\n",
    "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, iou, title):\n",
    "\n",
    "  n = 10\n",
    "\n",
    "  indexes = np.random.choice(len(predictions), size=n)\n",
    "  n_digits = digits[indexes]\n",
    "  n_predictions = predictions[indexes]\n",
    "  n_labels = labels[indexes]\n",
    "\n",
    "  n_iou = []\n",
    "  if len(iou) > 0:\n",
    "    n_iou = iou[indexes]\n",
    "\n",
    "  if (len(pred_bboxes) > 0):\n",
    "    n_pred_bboxes = pred_bboxes[indexes,:]\n",
    "\n",
    "  if (len(bboxes) > 0):\n",
    "    n_bboxes = bboxes[indexes,:]\n",
    "\n",
    "\n",
    "  n_digits = n_digits * 255.0\n",
    "  n_digits = n_digits.reshape(n, 75, 75)\n",
    "  fig = plt.figure(figsize=(20, 4))\n",
    "  plt.title(title)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  \n",
    "  for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i+1)\n",
    "    bboxes_to_plot = []\n",
    "    if (len(pred_bboxes) > i):\n",
    "      bboxes_to_plot.append(n_pred_bboxes[i])\n",
    "    \n",
    "    if (len(bboxes) > i):\n",
    "      bboxes_to_plot.append(n_bboxes[i])\n",
    "\n",
    "    img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes=np.asarray(bboxes_to_plot), color=['red', 'green'], display_str_list=[\"true\", \"pred\"])\n",
    "    plt.xlabel(n_predictions[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    if n_predictions[i] != n_labels[i]:\n",
    "      ax.xaxis.label.set_color('red')\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.imshow(img_to_draw)\n",
    "\n",
    "    if len(iou) > i :\n",
    "      color = \"black\"\n",
    "      if (n_iou[i][0] < iou_threshold):\n",
    "        color = \"red\"\n",
    "      ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i][0]), color=color, transform=ax.transAxes)\n",
    "\n",
    "\n",
    "# utility to display training and validation curves\n",
    "def plot_metrics(metric_name, title, ylim=5):\n",
    "  plt.title(title)\n",
    "  plt.ylim(0,ylim)\n",
    "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
    "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lvo0t7XVIkWZ"
   },
   "source": [
    "## Strategy & Parameters\n",
    "\n",
    "The global batch size is the batch size per replica (64 in this case) times the number of replicas in the distribution strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cCpkS9C_H7Tl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n",
      "Number of accelerators:  1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "print('Running on CPU')\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync # Gobal batch size.\n",
    "# The global batch size will be automatically sharded across all\n",
    "# replicas by the tf.data.Dataset API. A single TPU has 8 cores.\n",
    "# The best practice is to scale the batch size by the number of\n",
    "# replicas (cores). The learning rate should be increased as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVkc7nzg-WUy"
   },
   "source": [
    "## Loading and Preprocessing the Dataset\n",
    "\n",
    "Define some helper functions that will pre-process your data:\n",
    "- `read_image_tfds`: randomly overlays the \"bees\" image on top of a larger canvas.\n",
    "- `get_training_dataset`: loads data and splits it to get the training set.\n",
    "- `get_validation_dataset`: loads and splits the data to get the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZE8dgyPC1_6m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Transforms each image in dataset by pasting it on a 75x75 canvas at random locations.\n",
    "'''\n",
    "def read_image_tfds(image, label):\n",
    "    xmin = tf.random.uniform((), 0 , 48, dtype=tf.int32)\n",
    "    ymin = tf.random.uniform((), 0 , 48, dtype=tf.int32)\n",
    "    image = tf.reshape(image, (28,28,1,))\n",
    "    image = tf.image.pad_to_bounding_box(image, ymin, xmin, 75, 75)\n",
    "    image = tf.cast(image, tf.float32)/255.0\n",
    "    xmin = tf.cast(xmin, tf.float32)\n",
    "    ymin = tf.cast(ymin, tf.float32)\n",
    "   \n",
    "    xmax = (xmin + 28) / 75\n",
    "    ymax = (ymin + 28) / 75\n",
    "    xmin = xmin / 75\n",
    "    ymin = ymin / 75\n",
    "    return image, (tf.one_hot(label, 10), [xmin, ymin, xmax, ymax])\n",
    "  \n",
    "'''\n",
    "Loads and maps the training split of the dataset using the map function. Note that we try to load the gcs version since TPU can only work with datasets on Google Cloud Storage.\n",
    "'''\n",
    "def get_training_dataset():\n",
    "      \n",
    "      with  strategy.scope():\n",
    "        dataset = tfds.load(\"mnist\", split=\"train\", as_supervised=True, try_gcs=True)\n",
    "        dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
    "        dataset = dataset.shuffle(5000, reshuffle_each_iteration=True)\n",
    "        dataset = dataset.repeat() # Mandatory for Keras for now\n",
    "        dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
    "        dataset = dataset.prefetch(-1)  # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
    "      return dataset\n",
    "\n",
    "'''\n",
    "Loads and maps the validation split of the dataset using the map function. Note that we try to load the gcs version since TPU can only work with datasets on Google Cloud Storage.\n",
    "'''  \n",
    "def get_validation_dataset():\n",
    "    dataset = tfds.load(\"mnist\", split=\"test\", as_supervised=True, try_gcs=True)\n",
    "    dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
    "\n",
    "    #dataset = dataset.cache() # this small dataset can be entirely cached in RAM\n",
    "    dataset = dataset.batch(10000, drop_remainder=True) # 10000 items in eval dataset, all in one batch\n",
    "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
    "    return dataset\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# DATA PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the datasets\n",
    "with strategy.scope():\n",
    "  training_dataset = get_training_dataset()\n",
    "  validation_dataset = get_validation_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSaXL28TZfk1"
   },
   "source": [
    "## Rubber Ducky data (Colab, TF3, C3, W2, Lab 3)\n",
    "\n",
    "We will start with some toy (literally) data consisting of 5 images of a rubber\n",
    "ducky.  Note that the [coco](https://cocodataset.org/#explore) dataset contains a number of animals, but notably, it does *not* contain rubber duckies (or even ducks for that matter), so this is a novel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQy3ND7EpFQM",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load images and visualize\n",
    "train_image_dir = 'models/research/object_detection/test_images/ducky/train/'\n",
    "train_images_np = []\n",
    "for i in range(1, 6):\n",
    "  image_path = os.path.join(train_image_dir, 'robertducky' + str(i) + '.jpg')\n",
    "  train_images_np.append(load_image_into_numpy_array(image_path))\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['xtick.labelsize'] = False\n",
    "plt.rcParams['ytick.labelsize'] = False\n",
    "plt.rcParams['xtick.top'] = False\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.right'] = False\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "\n",
    "for idx, train_image_np in enumerate(train_images_np):\n",
    "  plt.subplot(2, 3, idx+1)\n",
    "  plt.imshow(train_image_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTP9AFqecUgS"
   },
   "source": [
    "## In case you didn't want to label... (Colab, TF3, C3, W2, Lab 3)\n",
    "\n",
    "Run this cell only if you didn't annotate images with bounding boxes above and\n",
    "would prefer to just use our preannotated boxes.  Don't forget\n",
    "to uncomment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIAT6ZUmdHOC",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "gt_boxes = [\n",
    "            np.array([[0.436, 0.591, 0.629, 0.712]], dtype=np.float32),\n",
    "            np.array([[0.539, 0.583, 0.73, 0.71]], dtype=np.float32),\n",
    "            np.array([[0.464, 0.414, 0.626, 0.548]], dtype=np.float32),\n",
    "            np.array([[0.313, 0.308, 0.648, 0.526]], dtype=np.float32),\n",
    "            np.array([[0.256, 0.444, 0.484, 0.629]], dtype=np.float32)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dqb_yjAo3cO_"
   },
   "source": [
    "## Prepare data for training (Colab, TF3, C3, W2, Lab 3)\n",
    "\n",
    "Below we add the class annotations (for simplicity, we assume a single class in this colab; though it should be straightforward to extend this to handle multiple classes).  We also convert everything to the format that the training\n",
    "loop below expects (e.g., everything converted to tensors, classes converted to one-hot representations, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWBqFVMcweF-",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# By convention, our non-background classes start counting at 1.  Given\n",
    "# that we will be predicting just one class, we will therefore assign it a\n",
    "# `class id` of 1.\n",
    "duck_class_id = 1\n",
    "num_classes = 5\n",
    "\n",
    "category_index = {duck_class_id: {'id': duck_class_id, 'name': 'rubber_ducky'}}\n",
    "\n",
    "# Convert class labels to one-hot; convert everything to tensors.\n",
    "# The `label_id_offset` here shifts all classes by a certain number of indices;\n",
    "# we do this here so that the model receives one-hot labels where non-background\n",
    "# classes start counting at the zeroth index.  This is ordinarily just handled\n",
    "# automatically in our training binaries, but we need to reproduce it here.\n",
    "label_id_offset = 1\n",
    "train_image_tensors = []\n",
    "gt_classes_one_hot_tensors = []\n",
    "gt_box_tensors = []\n",
    "for (train_image_np, gt_box_np) in zip(\n",
    "    train_images_np, gt_boxes):\n",
    "  train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
    "      train_image_np, dtype=tf.float32), axis=0))\n",
    "  gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
    "  zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
    "      np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
    "  gt_classes_one_hot_tensors.append(tf.one_hot(\n",
    "      zero_indexed_groundtruth_classes, num_classes))\n",
    "print('Done prepping data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KBz-vFbjLZX"
   },
   "source": [
    "## Data augmentation (Colab, TF1 Public, C2, W2, Lab 1)\n",
    "\n",
    "One simple method to avoid overfitting is to augment the images a bit. If you think about it, most pictures of a cat are very similar -- the ears are at the top, then the eyes, then the mouth etc. Things like the distance between the eyes and ears will always be quite similar too. \n",
    "\n",
    "What if you tweak with the images a bit -- rotate the image, squash it, etc.  That's what image augementation is all about. And there's an API that makes it easy!\n",
    "\n",
    "Take a look at the [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) which you have been using to rescale the image. There are other properties on it that you can use to augment the image. \n",
    "\n",
    "```\n",
    "# Updated to do image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "```\n",
    "\n",
    "These are just a few of the options available. Let's quickly go over it:\n",
    "\n",
    "* `rotation_range` is a value in degrees (0–180) within which to randomly rotate pictures.\n",
    "* `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n",
    "* `shear_range` is for randomly applying shearing transformations.\n",
    "* `zoom_range` is for randomly zooming inside pictures.\n",
    "* `horizontal_flip` is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures).\n",
    "* `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n",
    "\n",
    "\n",
    "Run the next cells to see the impact on the results. The code is similar to the baseline but the definition of `train_datagen` has been updated to use the parameters described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UK7_Fflgv8YC"
   },
   "outputs": [],
   "source": [
    "# Create new model\n",
    "model_for_aug = create_model()\n",
    "\n",
    "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
    "# the image, we also rotate and do other operations\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "# Train the new model\n",
    "history_with_aug = model_for_aug.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=EPOCHS,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,  # 1000 images = batch_size * steps\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3_Z3mJWN9KJ"
   },
   "source": [
    "## Visualize as a sanity check (Colab, TF3, C3, W2, Lab 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBD6l-E4N71y",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dummy_scores = np.array([1.0], dtype=np.float32)  # give boxes a score of 100%\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "for idx in range(5):\n",
    "  plt.subplot(2, 3, idx+1)\n",
    "  plot_detections(\n",
    "      train_images_np[idx],\n",
    "      gt_boxes[idx],\n",
    "      np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
    "      dummy_scores, category_index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghDAsqfoZvPh"
   },
   "source": [
    "## Create model and restore weights for all but last layer (Colab, TF3, C3, W2, Lab 3)\n",
    "\n",
    "In this cell we build a single stage detection architecture (RetinaNet) and restore all but the classification layer at the top (which will be automatically randomly initialized).\n",
    "\n",
    "For simplicity, we have hardcoded a number of things in this colab for the specific RetinaNet architecture at hand (including assuming that the image size will always be 640x640), however it is not difficult to generalize to other model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J16r3NChD-7",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
    "\n",
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyT4BUbaMeG-",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print('Building model and restoring weights for fine-tuning...', flush=True)\n",
    "num_classes = 1\n",
    "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
    "\n",
    "# Load pipeline config and build a detection model.\n",
    "#\n",
    "# Since we are working off of a COCO architecture which predicts 90\n",
    "# class slots by default, we override the `num_classes` field here to be just\n",
    "# one (for our new rubber ducky class).\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "model_config.ssd.num_classes = num_classes\n",
    "model_config.ssd.freeze_batchnorm = True\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=True)\n",
    "\n",
    "# Set up object-based checkpoint restore --- RetinaNet has two prediction\n",
    "# `heads` --- one for classification, the other for box regression.  We will\n",
    "# restore the box regression head but initialize the classification head\n",
    "# from scratch (we show the omission below by commenting out the line that\n",
    "# we would add if we wanted to restore both heads)\n",
    "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
    "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
    "    #    (i.e., the classification head that we *will not* restore)\n",
    "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
    "    )\n",
    "fake_model = tf.compat.v2.train.Checkpoint(\n",
    "          _feature_extractor=detection_model._feature_extractor,\n",
    "          _box_predictor=fake_box_predictor)\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
    "ckpt.restore(checkpoint_path).expect_partial()\n",
    "\n",
    "# Run model through a dummy image so that variables are created\n",
    "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
    "prediction_dict = detection_model.predict(image, shapes)\n",
    "_ = detection_model.postprocess(prediction_dict, shapes)\n",
    "print('Weights restored!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCkWmdoZZ0zJ"
   },
   "source": [
    "## Eager mode custom training loop (Colab, TF3, C3, W2, Lab 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyHoF4mUrv5-",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "# These parameters can be tuned; since our training set has 5 images\n",
    "# it doesn't make sense to have a much larger batch size, though we could\n",
    "# fit more examples in memory if we wanted to.\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "num_batches = 100\n",
    "\n",
    "# Select variables in top layers to fine-tune.\n",
    "trainable_variables = detection_model.trainable_variables\n",
    "to_fine_tune = []\n",
    "prefixes_to_train = [\n",
    "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
    "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
    "for var in trainable_variables:\n",
    "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
    "    to_fine_tune.append(var)\n",
    "\n",
    "# Set up forward + backward pass for a single train step.\n",
    "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
    "  \"\"\"Get a tf.function for training step.\"\"\"\n",
    "\n",
    "  # Use tf.function for a bit of speed.\n",
    "  # Comment out the tf.function decorator if you want the inside of the\n",
    "  # function to run eagerly.\n",
    "  @tf.function\n",
    "  def train_step_fn(image_tensors,\n",
    "                    groundtruth_boxes_list,\n",
    "                    groundtruth_classes_list):\n",
    "    \"\"\"A single training iteration.\n",
    "\n",
    "    Args:\n",
    "      image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
    "        Note that the height and width can vary across images, as they are\n",
    "        reshaped within this function to be 640x640.\n",
    "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
    "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
    "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
    "        with type tf.float32 representing groundtruth boxes for each image in\n",
    "        the batch.\n",
    "\n",
    "    Returns:\n",
    "      A scalar tensor representing the total loss for the input batch.\n",
    "    \"\"\"\n",
    "    shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
    "    model.provide_groundtruth(\n",
    "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
    "        groundtruth_classes_list=groundtruth_classes_list)\n",
    "    with tf.GradientTape() as tape:\n",
    "      preprocessed_images = tf.concat(\n",
    "          [detection_model.preprocess(image_tensor)[0]\n",
    "           for image_tensor in image_tensors], axis=0)\n",
    "      prediction_dict = model.predict(preprocessed_images, shapes)\n",
    "      losses_dict = model.loss(prediction_dict, shapes)\n",
    "      total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "      gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
    "      optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
    "    return total_loss\n",
    "\n",
    "  return train_step_fn\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "train_step_fn = get_model_train_step_function(\n",
    "    detection_model, optimizer, to_fine_tune)\n",
    "\n",
    "print('Start fine-tuning!', flush=True)\n",
    "for idx in range(num_batches):\n",
    "  # Grab keys for a random subset of examples\n",
    "  all_keys = list(range(len(train_images_np)))\n",
    "  random.shuffle(all_keys)\n",
    "  example_keys = all_keys[:batch_size]\n",
    "\n",
    "  # Note that we do not do data augmentation in this demo.  If you want a\n",
    "  # a fun exercise, we recommend experimenting with random horizontal flipping\n",
    "  # and random cropping :)\n",
    "  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
    "  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
    "  image_tensors = [train_image_tensors[key] for key in example_keys]\n",
    "\n",
    "  # Training step (forward pass + backwards pass)\n",
    "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
    "\n",
    "  if idx % 10 == 0:\n",
    "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
    "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
    "\n",
    "print('Done fine-tuning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHlXL1x_Z3tc"
   },
   "source": [
    "## Load test images and run inference with new model (Colab, TF3, C3, W2, Lab 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcE6OwrHQJya",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test_image_dir = 'models/research/object_detection/test_images/ducky/test/'\n",
    "test_images_np = []\n",
    "for i in range(1, 50):\n",
    "  image_path = os.path.join(test_image_dir, 'out' + str(i) + '.jpg')\n",
    "  test_images_np.append(np.expand_dims(\n",
    "      load_image_into_numpy_array(image_path), axis=0))\n",
    "\n",
    "# Again, uncomment this decorator if you want to run inference eagerly\n",
    "@tf.function\n",
    "def detect(input_tensor):\n",
    "  \"\"\"Run detection on an input image.\n",
    "\n",
    "  Args:\n",
    "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
    "      Note that height and width can be anything since the image will be\n",
    "      immediately resized according to the needs of the model within this\n",
    "      function.\n",
    "\n",
    "  Returns:\n",
    "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
    "      and `detection_scores`).\n",
    "  \"\"\"\n",
    "  preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
    "  prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
    "  return detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "# Note that the first frame will trigger tracing of the tf.function, which will\n",
    "# take some time, after which inference should be fast.\n",
    "\n",
    "label_id_offset = 1\n",
    "for i in range(len(test_images_np)):\n",
    "  input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
    "  detections = detect(input_tensor)\n",
    "\n",
    "  plot_detections(\n",
    "      test_images_np[i][0],\n",
    "      detections['detection_boxes'][0].numpy(),\n",
    "      detections['detection_classes'][0].numpy().astype(np.uint32)\n",
    "      + label_id_offset,\n",
    "      detections['detection_scores'][0].numpy(),\n",
    "      category_index, figsize=(15, 20), image_name=\"gif_frame_\" + ('%02d' % i) + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW1FrT2iNnpy",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "imageio.plugins.freeimage.download()\n",
    "\n",
    "anim_file = 'duckies_test.gif'\n",
    "\n",
    "filenames = glob.glob('gif_frame_*.jpg')\n",
    "filenames = sorted(filenames)\n",
    "last = -1\n",
    "images = []\n",
    "for filename in filenames:\n",
    "  image = imageio.imread(filename)\n",
    "  images.append(image)\n",
    "\n",
    "imageio.mimsave(anim_file, images, 'GIF-FI', fps=5)\n",
    "\n",
    "display(IPyImage(open(anim_file, 'rb').read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fXo6GuvL3EB"
   },
   "source": [
    "## Visualize Data (Colab, TF3, C3, W1, Lab 3)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZ4tjPKvL2eh"
   },
   "outputs": [],
   "source": [
    "(training_digits, training_labels, training_bboxes,\n",
    " validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(training_dataset, validation_dataset, 10)\n",
    "\n",
    "display_digits_with_boxes(training_digits, training_labels, training_labels, np.array([]), training_bboxes, np.array([]), \"training digits and their labels\")\n",
    "display_digits_with_boxes(validation_digits, validation_labels, validation_labels, np.array([]), validation_bboxes, np.array([]), \"validation digits and their labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FBn4V5-Krkt"
   },
   "source": [
    "## Intersection over union (Colab, TF3, C3, W1, Lab 3)¶\n",
    "\n",
    "Calculate the I-O-U metric to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFqJxt3_VrCm"
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(pred_box, true_box):\n",
    "    xmin_pred, ymin_pred, xmax_pred, ymax_pred =  np.split(pred_box, 4, axis = 1)\n",
    "    xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, 4, axis = 1)\n",
    "\n",
    "    smoothing_factor = 1e-10\n",
    "\n",
    "    xmin_overlap = np.maximum(xmin_pred, xmin_true)\n",
    "    xmax_overlap = np.minimum(xmax_pred, xmax_true)\n",
    "    ymin_overlap = np.maximum(ymin_pred, ymin_true)\n",
    "    ymax_overlap = np.minimum(ymax_pred, ymax_true)\n",
    "\n",
    "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
    "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
    "\n",
    "    overlap_area = np.maximum((xmax_overlap - xmin_overlap), 0)  * np.maximum((ymax_overlap - ymin_overlap), 0)\n",
    "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
    "    \n",
    "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jFVovcUUVs1"
   },
   "source": [
    "## Visualize predictions (Colab, TF3, C3, W1, Lab 3)¶\n",
    "The following code will make predictions and visualize both the classification and the predicted bounding boxes.\n",
    "- The true bounding box labels will be in green, and the model's predicted bounding boxes are in red.\n",
    "- The predicted number is shown below the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w12OId8Mz7dF"
   },
   "outputs": [],
   "source": [
    "# recognize validation digits\n",
    "predictions = model.predict(validation_digits, batch_size=64)\n",
    "predicted_labels = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "predicted_bboxes = predictions[1]\n",
    "\n",
    "iou = intersection_over_union(predicted_bboxes, validation_bboxes)\n",
    "\n",
    "iou_threshold = 0.6\n",
    "\n",
    "print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\n",
    "print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))\n",
    "\n",
    "\n",
    "display_digits_with_boxes(validation_digits, predicted_labels, validation_labels, predicted_bboxes, validation_bboxes, iou, \"True and Predicted values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# ARCHIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8nHWWkS_eeZ"
   },
   "source": [
    "## Define the Network (Colab, TF3, C3, W1, Lab 3)¶\n",
    "\n",
    "Here, you'll define your custom CNN. \n",
    "- `feature_extractor`: these convolutional layers extract the features of the image.\n",
    "- `classifier`:  This define the output layer that predicts among 10 categories (digits 0 through 9)\n",
    "- `bounding_box_regression`: This defines the output layer that predicts 4 numeric values, which define the coordinates of the bounding box (xmin, ymin, xmax, ymax)\n",
    "- `final_model`: This combines the layers for feature extraction, classification and bounding box prediction.  \n",
    "  - Notice that this is another example of a branching model, because the model splits to produce two kinds of output (a category and set of numbers).  \n",
    "  - Since you've learned to use the Functional API earlier in the specialization (course 1), you have the flexibility to define this kind of branching model!\n",
    "- `define_and_compile_model`: choose the optimizer and metrics, then compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56y8UNFQIVwj"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Feature extractor is the CNN that is made up of convolution and pooling layers.\n",
    "'''\n",
    "def feature_extractor(inputs):\n",
    "    x = tf.keras.layers.Conv2D(16, activation='relu', kernel_size=3, input_shape=(75, 75, 1))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "'''\n",
    "dense_layers adds a flatten and dense layer.\n",
    "This will follow the feature extraction layers\n",
    "'''\n",
    "def dense_layers(inputs):\n",
    "  x = tf.keras.layers.Flatten()(inputs)\n",
    "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "'''\n",
    "Classifier defines the classification output.\n",
    "This has a set of fully connected layers and a softmax layer.\n",
    "'''\n",
    "def classifier(inputs):\n",
    "\n",
    "  classification_output = tf.keras.layers.Dense(10, activation='softmax', name = 'classification')(inputs)\n",
    "  return classification_output\n",
    "\n",
    "\n",
    "'''\n",
    "This function defines the regression output for bounding box prediction. \n",
    "Note that we have four outputs corresponding to (xmin, ymin, xmax, ymax)\n",
    "'''\n",
    "def bounding_box_regression(inputs):\n",
    "    bounding_box_regression_output = tf.keras.layers.Dense(units = '4', name = 'bounding_box')(inputs)\n",
    "    return bounding_box_regression_output\n",
    "\n",
    "\n",
    "def final_model(inputs):\n",
    "    feature_cnn = feature_extractor(inputs)\n",
    "    dense_output = dense_layers(feature_cnn)\n",
    "\n",
    "    '''\n",
    "    The model branches here.  \n",
    "    The dense layer's output gets fed into two branches:\n",
    "    classification_output and bounding_box_output\n",
    "    '''\n",
    "    classification_output = classifier(dense_output)\n",
    "    bounding_box_output = bounding_box_regression(dense_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = [classification_output, bounding_box_output])\n",
    "\n",
    "    return model\n",
    "  \n",
    "\n",
    "def define_and_compile_model(inputs):\n",
    "  model = final_model(inputs)\n",
    "  \n",
    "  model.compile(optimizer='adam', \n",
    "              loss = {'classification' : 'categorical_crossentropy',\n",
    "                      'bounding_box' : 'mse'\n",
    "                     },\n",
    "              metrics = {'classification' : 'accuracy',\n",
    "                         'bounding_box' : 'mse'\n",
    "                        })\n",
    "  return model\n",
    "\n",
    "    \n",
    "with strategy.scope():\n",
    "  inputs = tf.keras.layers.Input(shape=(75, 75, 1,))\n",
    "  model = define_and_compile_model(inputs)\n",
    "\n",
    "# print model layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv0BQTPsKrkt"
   },
   "source": [
    "## Train and validate the model (Colab, TF3, C3, W1, Lab 3)¶\n",
    "\n",
    "Train the model.  \n",
    "- You can choose the number of epochs depending on the level of performance that you want and the time that you have.\n",
    "- Each epoch will take just a few seconds if you're using the TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTwH_P-ZJ_xx"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10 # 45\n",
    "steps_per_epoch = 60000//BATCH_SIZE  # 60,000 items in this dataset\n",
    "validation_steps = 1\n",
    "\n",
    "history = model.fit(training_dataset,\n",
    "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)\n",
    "\n",
    "loss, classification_loss, bounding_box_loss, classification_accuracy, bounding_box_mse = model.evaluate(validation_dataset, steps=1)\n",
    "print(\"Validation accuracy: \", classification_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz-b8TxU6EDj"
   },
   "outputs": [],
   "source": [
    "plot_metrics(\"classification_loss\", \"Classification Loss\")\n",
    "plot_metrics(\"bounding_box_loss\", \"Bounding Box Loss\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "interactive_eager_few_shot_od_training_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
