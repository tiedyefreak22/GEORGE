{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QKNrXG_bMlFf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "\n",
    "from tensorflow import data as tf_data\n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "from keras_cv import bounding_box\n",
    "import os\n",
    "from keras_cv import visualization\n",
    "import tqdm\n",
    "from GEORGE_Library import *\n",
    "import win32file\n",
    "win32file._setmaxstdio(2048)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "category_index = {1: {'id': 1, 'name': 'regular'}, 2: {'id': 2, 'name': 'pollen'}, 3: {'id': 3, 'name': 'varroa'}, 4: {'id': 4, 'name': 'wasps'}}\n",
    "train_augmented_fp = \"Dataset/Custom_Dataset/Train\"\n",
    "val_augmented_fp = \"Dataset/Custom_Dataset/Validation\"\n",
    "model_directory = 'C:/Users/khard/OneDrive/Documents/GitHub/GEORGE/'\n",
    "model_name = 'extract_superimp_model'\n",
    "model_dest = os.path.join(os.sep, model_directory, model_name)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "table = {'regular': 1, 'pollen': 2, 'varroa': 3, 'wasps': 4}\n",
    "class_mapping = {1: 'regular', 2: 'pollen', 3: 'varroa', 4: 'wasps'}\n",
    "\n",
    "splits = {\n",
    "    'train': 'Dataset/Custom_Dataset/Train',\n",
    "    'test': 'Dataset/Custom_Dataset/Test',\n",
    "    'validation': 'Dataset/Custom_Dataset/Validation'\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def load(*, split, bounding_box_format):\n",
    "    if not split in splits:\n",
    "        raise ValueError(\n",
    "            f\"Invalid split provided, `split={split}`. \"\n",
    "            f\"Expected one of {list(splits.keys())}\"\n",
    "        )\n",
    "\n",
    "    path = splits[split]\n",
    "    coco = COCOParser(f'{path}/custom_bee_dataset.json', path)\n",
    "    # file_annotations = json.load(f)\n",
    "\n",
    "    def generator():\n",
    "        for i, im in enumerate(coco.get_imgIds()):\n",
    "            image_path = f\"{path}/{str(im).zfill(0)}.png\"\n",
    "            ann_ids = coco.get_annIds(im)\n",
    "            annotations = coco.load_anns(ann_ids)\n",
    "\n",
    "            box_labels = []\n",
    "            class_labels = []\n",
    "            \n",
    "            for ann in annotations:\n",
    "                _bbox = ann['bbox']\n",
    "                x, y, w, h = [int(b) for b in _bbox]\n",
    "                box = tf.constant(\n",
    "                    [x, y, w, h], tf.float32\n",
    "                )\n",
    "                box_labels.append(\n",
    "                  box\n",
    "                )\n",
    "    \n",
    "                class_id = ann[\"category_id\"]\n",
    "                class_name = coco.load_cats(class_id)[0][\"name\"]\n",
    "\n",
    "                class_labels.append(\n",
    "                    tf.constant(lut(class_name), tf.float32)\n",
    "                )\n",
    "            if len(box_labels) == 0:\n",
    "                continue\n",
    "            bounding_boxes = {\n",
    "                'boxes': tf.stack(box_labels),\n",
    "                'classes': tf.stack(class_labels)\n",
    "            }\n",
    "            image = load_image(f\"{image_path}\")\n",
    "            bounding_boxes = keras_cv.bounding_box.convert_format(bounding_boxes, source ='xywh', target=bounding_box_format)\n",
    "            yield {\n",
    "                'images': image,\n",
    "                'bounding_boxes': bounding_boxes\n",
    "            }\n",
    "    output_spec = {\n",
    "        'images': tf.TensorSpec(shape=(None, None, 3)),\n",
    "        'bounding_boxes': {\n",
    "            'boxes': tf.TensorSpec(shape=(None, 4)),\n",
    "            'classes': tf.TensorSpec(shape=(None,))\n",
    "        }\n",
    "    }\n",
    "    return tf.data.Dataset.from_generator(generator, output_signature=output_spec)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XvOYLTR3MlFn"
   },
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} NameError: name 'lut' is not defined\nTraceback (most recent call last):\n\n  File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 268, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\khard\\AppData\\Local\\Temp\\ipykernel_14112\\970152679.py\", line 46, in generator\n    tf.constant(lut(class_name), tf.float32)\n\nNameError: name 'lut' is not defined. Did you mean: 'Out'?\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mprefetch(tf_data\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m     13\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m val_dataset\u001b[38;5;241m.\u001b[39mprefetch(tf_data\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mvisualize_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounding_box_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxywh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GitHub\\GEORGE\\GEORGE_Library.py:570\u001b[0m, in \u001b[0;36mvisualize_dataset\u001b[1;34m(inputs, value_range, rows, cols, bounding_box_format)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_dataset\u001b[39m(inputs, value_range, rows, cols, bounding_box_format):\n\u001b[1;32m--> 570\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m     images, bounding_boxes \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m], inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbounding_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    572\u001b[0m     visualization\u001b[38;5;241m.\u001b[39mplot_bounding_box_gallery(\n\u001b[0;32m    573\u001b[0m         images\u001b[38;5;241m.\u001b[39mto_tensor(),\n\u001b[0;32m    574\u001b[0m         value_range\u001b[38;5;241m=\u001b[39mvalue_range,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    581\u001b[0m         class_mapping\u001b[38;5;241m=\u001b[39mclass_mapping,\n\u001b[0;32m    582\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:814\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    813\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:777\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 777\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3055\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3053\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3055\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3057\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   6655\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6656\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} NameError: name 'lut' is not defined\nTraceback (most recent call last):\n\n  File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 268, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\khard\\AppData\\Local\\Temp\\ipykernel_14112\\970152679.py\", line 46, in generator\n    tf.constant(lut(class_name), tf.float32)\n\nNameError: name 'lut' is not defined. Did you mean: 'Out'?\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "train_dataset = load(split=\"train\", bounding_box_format=\"xywh\")\n",
    "val_dataset = load(split=\"test\", bounding_box_format=\"xywh\")\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BATCH_SIZE * 4)\n",
    "\n",
    "train_dataset = train_dataset.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "train_dataset = train_dataset.map(dict_to_tuple, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(dict_to_tuple, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.prefetch(tf_data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(tf_data.AUTOTUNE)\n",
    "\n",
    "visualize_dataset(\n",
    "    train_dataset, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9Xun7KsMlFv"
   },
   "source": [
    "### Optimizer\n",
    "\n",
    "In this guide, we use a standard SGD optimizer and rely on the\n",
    "[`keras.callbacks.ReduceLROnPlateau`](https://keras.io/api/callbacks/reduce_lr_on_plateau/)\n",
    "callback to reduce the learning rate.\n",
    "\n",
    "You will always want to include a `global_clipnorm` when training object\n",
    "detection models.  This is to remedy exploding gradient problems that frequently\n",
    "occur when training object detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d4LzB1FcMlFv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.005\n",
    "# including a global_clipnorm is extremely important in object detection tasks\n",
    "optimizer = keras.optimizers.SGD(\n",
    "    learning_rate=base_lr, momentum=0.9, global_clipnorm=10.0\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_V0QWIwMlFw"
   },
   "source": [
    "To achieve the best results on your dataset, you'll likely want to hand craft a\n",
    "`PiecewiseConstantDecay` learning rate schedule.\n",
    "While `PiecewiseConstantDecay` schedules tend to perform better, they don't\n",
    "translate between problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcwKGT8DMlFw"
   },
   "source": [
    "### Loss functions\n",
    "\n",
    "You may not be familiar with the `\"ciou\"` loss.  While not common in other\n",
    "models, this loss is sometimes used in the object detection world.\n",
    "\n",
    "In short, [\"Complete IoU\"](https://arxiv.org/abs/1911.08287) is a flavour of the Intersection over Union loss and is used due to its convergence properties.\n",
    "\n",
    "In KerasCV, you can use this loss simply by passing the string `\"ciou\"` to `compile()`.\n",
    "We also use standard binary crossentropy loss for the class head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Byb454j5MlFw"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpretrained_model\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      2\u001b[0m     classification_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     box_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mciou\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pretrained_model' is not defined"
     ]
    }
   ],
   "source": [
    "# pretrained_model.compile(\n",
    "#     classification_loss=\"binary_crossentropy\",\n",
    "#     box_loss=\"ciou\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE-otqPIMlFx"
   },
   "source": [
    "### Metric evaluation\n",
    "\n",
    "The most popular object detection metrics are COCO metrics,\n",
    "which were published alongside the MSCOCO dataset. KerasCV provides an\n",
    "easy-to-use suite of COCO metrics under the `keras_cv.callbacks.PyCOCOCallback`\n",
    "symbol. Note that we use a Keras callback instead of a Keras metric to compute\n",
    "COCO metrics. This is because computing COCO metrics requires storing all of a\n",
    "model's predictions for the entire evaluation dataset in memory at once, which\n",
    "is impractical to do during training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fak6CQe3MlFx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "coco_metrics_callback = keras_cv.callbacks.PyCOCOCallback(\n",
    "    val_dataset.take(20), bounding_box_format=\"xywh\"\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AVLtnOdMlFx"
   },
   "source": [
    "Our data pipeline is now complete!\n",
    "We can now move on to model creation and training.\n",
    "\n",
    "## Model creation\n",
    "\n",
    "Next, let's use the KerasCV API to construct an untrained YOLOV8Detector model.\n",
    "In this tutorial we use a pretrained ResNet50 backbone from the imagenet\n",
    "dataset.\n",
    "\n",
    "KerasCV makes it easy to construct a `YOLOV8Detector` with any of the KerasCV\n",
    "backbones.  Simply use one of the presets for the architecture you'd like!\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\khard\\AppData\\Local\\Temp\\__autograph_generated_filezdeh9sfm.py\", line 10, in tf__call\n        features = ag__.converted_call(ag__.ld(self).fpn, (ag__.ld(image),), dict(training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\khard\\AppData\\Local\\Temp\\__autograph_generated_fileq6zo6991.py\", line 10, in tf__call\n        (c3_output, c4_output, c5_output) = ag__.converted_call(ag__.ld(self).backbone, (ag__.ld(images),), dict(training=ag__.ld(training)), fscope)\n\n    ValueError: Exception encountered when calling layer 'RetinaNet' (type RetinaNet).\n    \n    in user code:\n    \n        File \"C:\\Users\\khard\\AppData\\Local\\Temp\\ipykernel_36060\\3610939781.py\", line 297, in call  *\n            features = self.fpn(image, training=training)\n        File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\khard\\AppData\\Local\\Temp\\__autograph_generated_fileq6zo6991.py\", line 10, in tf__call\n            (c3_output, c4_output, c5_output) = ag__.converted_call(ag__.ld(self).backbone, (ag__.ld(images),), dict(training=ag__.ld(training)), fscope)\n    \n        ValueError: Exception encountered when calling layer 'FeaturePyramid' (type FeaturePyramid).\n        \n        in user code:\n        \n            File \"C:\\Users\\khard\\AppData\\Local\\Temp\\ipykernel_36060\\3610939781.py\", line 235, in call  *\n                c3_output, c4_output, c5_output = self.backbone(images, training=training)\n            File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 197, in assert_input_compatibility\n                raise ValueError(\n        \n            ValueError: Missing data for input \"input_10\". You passed a data dictionary with keys ['images', 'bounding_boxes']. Expected the following keys: ['input_10']\n        \n        \n        Call arguments received by layer 'FeaturePyramid' (type FeaturePyramid):\n          • images={'images': 'tf.Tensor(shape=(None, None, 3), dtype=float32)', 'bounding_boxes': {'boxes': 'tf.Tensor(shape=(None, 4), dtype=float32)', 'classes': 'tf.Tensor(shape=(None,), dtype=float32)'}}\n          • training=True\n    \n    \n    Call arguments received by layer 'RetinaNet' (type RetinaNet):\n      • image={'images': 'tf.Tensor(shape=(None, None, 3), dtype=float32)', 'bounding_boxes': {'boxes': 'tf.Tensor(shape=(None, 4), dtype=float32)', 'classes': 'tf.Tensor(shape=(None,), dtype=float32)'}}\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 514\u001b[0m\n\u001b[0;32m    509\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m# Running 100 training and 50 validation steps,\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# remove `.take` when training on the full dataset\u001b[39;00m\n\u001b[1;32m--> 514\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file8gyw2h58.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filezdeh9sfm.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, image, training)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m N \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(image),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m cls_outputs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileq6zo6991.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, images, training)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m (c3_output, c4_output, c5_output) \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m p3_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconv_c3_1x1, (ag__\u001b[38;5;241m.\u001b[39mld(c3_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m p4_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconv_c4_1x1, (ag__\u001b[38;5;241m.\u001b[39mld(c4_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\khard\\AppData\\Local\\Temp\\__autograph_generated_filezdeh9sfm.py\", line 10, in tf__call\n        features = ag__.converted_call(ag__.ld(self).fpn, (ag__.ld(image),), dict(training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\khard\\AppData\\Local\\Temp\\__autograph_generated_fileq6zo6991.py\", line 10, in tf__call\n        (c3_output, c4_output, c5_output) = ag__.converted_call(ag__.ld(self).backbone, (ag__.ld(images),), dict(training=ag__.ld(training)), fscope)\n\n    ValueError: Exception encountered when calling layer 'RetinaNet' (type RetinaNet).\n    \n    in user code:\n    \n        File \"C:\\Users\\khard\\AppData\\Local\\Temp\\ipykernel_36060\\3610939781.py\", line 297, in call  *\n            features = self.fpn(image, training=training)\n        File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\khard\\AppData\\Local\\Temp\\__autograph_generated_fileq6zo6991.py\", line 10, in tf__call\n            (c3_output, c4_output, c5_output) = ag__.converted_call(ag__.ld(self).backbone, (ag__.ld(images),), dict(training=ag__.ld(training)), fscope)\n    \n        ValueError: Exception encountered when calling layer 'FeaturePyramid' (type FeaturePyramid).\n        \n        in user code:\n        \n            File \"C:\\Users\\khard\\AppData\\Local\\Temp\\ipykernel_36060\\3610939781.py\", line 235, in call  *\n                c3_output, c4_output, c5_output = self.backbone(images, training=training)\n            File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 197, in assert_input_compatibility\n                raise ValueError(\n        \n            ValueError: Missing data for input \"input_10\". You passed a data dictionary with keys ['images', 'bounding_boxes']. Expected the following keys: ['input_10']\n        \n        \n        Call arguments received by layer 'FeaturePyramid' (type FeaturePyramid):\n          • images={'images': 'tf.Tensor(shape=(None, None, 3), dtype=float32)', 'bounding_boxes': {'boxes': 'tf.Tensor(shape=(None, 4), dtype=float32)', 'classes': 'tf.Tensor(shape=(None,), dtype=float32)'}}\n          • training=True\n    \n    \n    Call arguments received by layer 'RetinaNet' (type RetinaNet):\n      • image={'images': 'tf.Tensor(shape=(None, None, 3), dtype=float32)', 'bounding_boxes': {'boxes': 'tf.Tensor(shape=(None, 4), dtype=float32)', 'classes': 'tf.Tensor(shape=(None,), dtype=float32)'}}\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes.\n",
    "\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
    "    format `[x, y, width, height]`.\n",
    "\n",
    "    Attributes:\n",
    "      aspect_ratios: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map\n",
    "      scales: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "      num_anchors: The number of anchor boxes at each location on feature map\n",
    "      areas: A list of float values representing the areas of the anchor\n",
    "        boxes for each feature map in the feature pyramid.\n",
    "      strides: A list of float value representing the strides for each feature\n",
    "        map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
    "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "\n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(3, 8)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\n",
    "        \"\"\"\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "\n",
    "        Arguments:\n",
    "          feature_height: An integer representing the height of the feature map.\n",
    "          feature_width: An integer representing the width of the feature map.\n",
    "          level: An integer representing the level of the feature map in the\n",
    "            feature pyramid.\n",
    "\n",
    "        Returns:\n",
    "          anchor boxes with the shape\n",
    "          `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(3, 8)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "\n",
    "class LabelEncoder:\n",
    "    \"\"\"Transforms the raw labels into targets for training.\n",
    "\n",
    "    This class has operations to generate targets for a batch of samples which\n",
    "    is made up of the input images, bounding boxes for the objects present and\n",
    "    their class ids.\n",
    "\n",
    "    Attributes:\n",
    "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
    "      box_variance: The scaling factors used to scale the bounding box targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
    "    ):\n",
    "        \"\"\"Matches ground truth boxes to anchor boxes based on IOU.\n",
    "\n",
    "        1. Calculates the pairwise IOU for the M `anchor_boxes` and N `gt_boxes`\n",
    "          to get a `(M, N)` shaped matrix.\n",
    "        2. The ground truth box with the maximum IOU in each row is assigned to\n",
    "          the anchor box provided the IOU is greater than `match_iou`.\n",
    "        3. If the maximum IOU in a row is less than `ignore_iou`, the anchor\n",
    "          box is assigned with the background class.\n",
    "        4. The remaining anchor boxes that do not have any class assigned are\n",
    "          ignored during training.\n",
    "\n",
    "        Arguments:\n",
    "          anchor_boxes: A float tensor with the shape `(total_anchors, 4)`\n",
    "            representing all the anchor boxes for a given input image shape,\n",
    "            where each anchor box is of the format `[x, y, width, height]`.\n",
    "          gt_boxes: A float tensor with shape `(num_objects, 4)` representing\n",
    "            the ground truth boxes, where each box is of the format\n",
    "            `[x, y, width, height]`.\n",
    "          match_iou: A float value representing the minimum IOU threshold for\n",
    "            determining if a ground truth box can be assigned to an anchor box.\n",
    "          ignore_iou: A float value representing the IOU threshold under which\n",
    "            an anchor box is assigned to the background class.\n",
    "\n",
    "        Returns:\n",
    "          matched_gt_idx: Index of the matched object\n",
    "          positive_mask: A mask for anchor boxes that have been assigned ground\n",
    "            truth boxes.\n",
    "          ignore_mask: A mask for anchor boxes that need to by ignored during\n",
    "            training\n",
    "        \"\"\"\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
    "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        cls_target = tf.where(\n",
    "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
    "        )\n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "        label = tf.concat([box_target, cls_target], axis=-1)\n",
    "        return label\n",
    "\n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "\n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            labels = labels.write(i, label)\n",
    "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
    "        return batch_images, labels.stack()\n",
    "\n",
    "def get_backbone():\n",
    "    \"\"\"Builds ResNet50 with pre-trained imagenet weights\"\"\"\n",
    "    backbone = keras.applications.ResNet50(\n",
    "        include_top=False, input_shape=[None, None, 3]\n",
    "    )\n",
    "    c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
    "    ]\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
    "    )\n",
    "\n",
    "class FeaturePyramid(keras.layers.Layer):\n",
    "    \"\"\"Builds the Feature Pyramid with the feature maps from the backbone.\n",
    "\n",
    "    Attributes:\n",
    "      num_classes: Number of classes in the dataset.\n",
    "      backbone: The backbone to build the feature pyramid from.\n",
    "        Currently supports ResNet50 only.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"FeaturePyramid\", **kwargs)\n",
    "        self.backbone = backbone if backbone else get_backbone()\n",
    "        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
    "        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
    "        self.upsample_2x = keras.layers.UpSampling2D(2)\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
    "        p3_output = self.conv_c3_1x1(c3_output)\n",
    "        p4_output = self.conv_c4_1x1(c4_output)\n",
    "        p5_output = self.conv_c5_1x1(c5_output)\n",
    "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
    "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
    "        p3_output = self.conv_c3_3x3(p3_output)\n",
    "        p4_output = self.conv_c4_3x3(p4_output)\n",
    "        p5_output = self.conv_c5_3x3(p5_output)\n",
    "        p6_output = self.conv_c6_3x3(c5_output)\n",
    "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
    "        return p3_output, p4_output, p5_output, p6_output, p7_output\n",
    "\n",
    "def build_head(output_filters, bias_init):\n",
    "    \"\"\"Builds the class/box predictions head.\n",
    "\n",
    "    Arguments:\n",
    "      output_filters: Number of convolution filters in the final layer.\n",
    "      bias_init: Bias Initializer for the final convolution layer.\n",
    "\n",
    "    Returns:\n",
    "      A keras sequential model representing either the classification\n",
    "        or the box regression head depending on `output_filters`.\n",
    "    \"\"\"\n",
    "    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n",
    "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
    "    for _ in range(4):\n",
    "        head.add(\n",
    "            keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n",
    "        )\n",
    "        head.add(keras.layers.ReLU())\n",
    "    head.add(\n",
    "        keras.layers.Conv2D(\n",
    "            output_filters,\n",
    "            3,\n",
    "            1,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            bias_initializer=bias_init,\n",
    "        )\n",
    "    )\n",
    "    return head\n",
    "\n",
    "class RetinaNet(keras.Model):\n",
    "    \"\"\"A subclassed Keras model implementing the RetinaNet architecture.\n",
    "\n",
    "    Attributes:\n",
    "      num_classes: Number of classes in the dataset.\n",
    "      backbone: The backbone to build the feature pyramid from.\n",
    "        Currently supports ResNet50 only.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"RetinaNet\", **kwargs)\n",
    "        self.fpn = FeaturePyramid(backbone)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
    "        self.box_head = build_head(9 * 4, \"zeros\")\n",
    "\n",
    "    def call(self, image, training=False):\n",
    "        features = self.fpn(image, training=training)\n",
    "        N = tf.shape(image)[0]\n",
    "        cls_outputs = []\n",
    "        box_outputs = []\n",
    "        for feature in features:\n",
    "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
    "            cls_outputs.append(\n",
    "                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
    "            )\n",
    "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "        box_outputs = tf.concat(box_outputs, axis=1)\n",
    "        return tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "\n",
    "class DecodePredictions(tf.keras.layers.Layer):\n",
    "    \"\"\"A Keras layer that decodes predictions of the RetinaNet model.\n",
    "\n",
    "    Attributes:\n",
    "      num_classes: Number of classes in the dataset\n",
    "      confidence_threshold: Minimum class probability, below which detections\n",
    "        are pruned.\n",
    "      nms_iou_threshold: IOU threshold for the NMS operation\n",
    "      max_detections_per_class: Maximum number of detections to retain per\n",
    "       class.\n",
    "      max_detections: Maximum number of detections to retain across all\n",
    "        classes.\n",
    "      box_variance: The scaling factors used to scale the bounding box\n",
    "        predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=80,\n",
    "        confidence_threshold=0.05,\n",
    "        nms_iou_threshold=0.5,\n",
    "        max_detections_per_class=100,\n",
    "        max_detections=100,\n",
    "        box_variance=[0.1, 0.1, 0.2, 0.2],\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.nms_iou_threshold = nms_iou_threshold\n",
    "        self.max_detections_per_class = max_detections_per_class\n",
    "        self.max_detections = max_detections\n",
    "\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
    "        boxes = box_predictions * self._box_variance\n",
    "        boxes = tf.concat(\n",
    "            [\n",
    "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        boxes_transformed = convert_to_corners(boxes)\n",
    "        return boxes_transformed\n",
    "\n",
    "    def call(self, images, predictions):\n",
    "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        box_predictions = predictions[:, :, :4]\n",
    "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
    "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "\n",
    "        return tf.image.combined_non_max_suppression(\n",
    "            tf.expand_dims(boxes, axis=2),\n",
    "            cls_predictions,\n",
    "            self.max_detections_per_class,\n",
    "            self.max_detections,\n",
    "            self.nms_iou_threshold,\n",
    "            self.confidence_threshold,\n",
    "            clip_boxes=False,\n",
    "        )\n",
    "\n",
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super().__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=y_true, logits=y_pred\n",
    "        )\n",
    "        probs = tf.nn.sigmoid(y_pred)\n",
    "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=80, alpha=0.25, gamma=2.0, delta=1.0):\n",
    "        super().__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
    "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
    "        self._box_loss = RetinaNetBoxLoss(delta)\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        box_labels = y_true[:, :, :4]\n",
    "        box_predictions = y_pred[:, :, :4]\n",
    "        cls_labels = tf.one_hot(\n",
    "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
    "            depth=self._num_classes,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        cls_predictions = y_pred[:, :, 4:]\n",
    "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
    "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
    "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions)\n",
    "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
    "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        loss = clf_loss + box_loss\n",
    "        return loss\n",
    "\n",
    "model_dir = \"retinanet/\"\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "num_classes = 4\n",
    "batch_size = 2\n",
    "\n",
    "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=learning_rate_boundaries, values=learning_rates\n",
    ")\n",
    "resnet50_backbone = get_backbone()\n",
    "loss_fn = RetinaNetLoss(num_classes)\n",
    "model = RetinaNet(num_classes, resnet50_backbone)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
    "model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "\n",
    "#  set `data_dir=None` to load the complete dataset\n",
    "\n",
    "# (train_dataset, val_dataset), dataset_info = tfds.load(\n",
    "#     \"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir=\"data\"\n",
    "# )\n",
    "\n",
    "# autotune = tf.data.AUTOTUNE\n",
    "# #train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "# train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "# train_dataset = train_dataset.padded_batch(\n",
    "#     batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
    "# )\n",
    "# train_dataset = train_dataset.map(\n",
    "#     label_encoder.encode_batch, num_parallel_calls=autotune\n",
    "# )\n",
    "# train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "# train_dataset = train_dataset.prefetch(autotune)\n",
    "\n",
    "# #val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "# val_dataset = val_dataset.padded_batch(\n",
    "#     batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
    "# )\n",
    "# val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "# val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "# val_dataset = val_dataset.prefetch(autotune)\n",
    "\n",
    "train_steps_per_epoch = 20000 // batch_size\n",
    "val_steps_per_epoch = \\\n",
    "    4250 // batch_size\n",
    "\n",
    "train_steps = 4 * 100000\n",
    "epochs = train_steps // train_steps_per_epoch\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Running 100 training and 50 validation steps,\n",
    "# remove `.take` when training on the full dataset\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset.take(50),\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pX6gdfsAMlFx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# model = keras_cv.models.YOLOV8Detector.from_preset(\n",
    "#     \"resnet50_imagenet\",\n",
    "#     # For more info on supported bounding box formats, visit\n",
    "#     # https://keras.io/api/keras_cv/bounding_box/\n",
    "#     bounding_box_format=\"xywh\",\n",
    "#     num_classes=4,\n",
    "# )\n",
    "\n",
    "# Load randomly initialized model from preset architecture with weights\n",
    "model = keras_cv.models.RetinaNet.from_preset(\n",
    "    \"resnet50_imagenet\",\n",
    "    load_weights=False,\n",
    "    bounding_box_format=\"xywh\",\n",
    "    num_classes=4,\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJeJPfH1MlFx"
   },
   "source": [
    "That is all it takes to construct a KerasCV YOLOv8. The YOLOv8 accepts\n",
    "tuples of dense image Tensors and bounding box dictionaries to `fit()` and\n",
    "`train_on_batch()`\n",
    "\n",
    "This matches what we have constructed in our input pipeline above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-62xZcWuMlFx"
   },
   "source": [
    "## Training our model\n",
    "\n",
    "All that is left to do is train our model.  KerasCV object detection models\n",
    "follow the standard Keras workflow, leveraging `compile()` and `fit()`.\n",
    "\n",
    "Let's compile our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "16npFQOvMlFy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    classification_loss=\"Focal\",\n",
    "    box_loss=\"Smoothl1\",\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQ-DZg7LMlFy"
   },
   "source": [
    "If you want to fully train the model, remove `.take(20)` from all dataset\n",
    "references (below and in the initialization of the metrics callback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3ty4_TWIMlFy"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras_cv\\src\\models\\object_detection\\retinanet\\retinanet.py\", line 468, in train_step\n        return super().train_step(*args, (x, y))\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer 'conv1_conv' (type Conv2D).\n    \n    Failed to convert elements of tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"retina_net_1/model_2/rescaling_3/Add:0\", shape=(None, 3), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(5,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n    \n    Call arguments received by layer 'conv1_conv' (type Conv2D):\n      • inputs=tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"retina_net_1/model_2/rescaling_3/Add:0\", shape=(None, 3), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(5,), dtype=int64))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcoco_metrics_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file6iodbedm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\keras_cv\\src\\models\\object_detection\\retinanet\\retinanet.py:468\u001b[0m, in \u001b[0;36mRetinaNet.train_step\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    466\u001b[0m args \u001b[38;5;241m=\u001b[39m args[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    467\u001b[0m x, y \u001b[38;5;241m=\u001b[39m unpack_input(data)\n\u001b[1;32m--> 468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras_cv\\src\\models\\object_detection\\retinanet\\retinanet.py\", line 468, in train_step\n        return super().train_step(*args, (x, y))\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\khard\\.conda\\envs\\tf2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer 'conv1_conv' (type Conv2D).\n    \n    Failed to convert elements of tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"retina_net_1/model_2/rescaling_3/Add:0\", shape=(None, 3), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(5,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n    \n    Call arguments received by layer 'conv1_conv' (type Conv2D):\n      • inputs=tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"retina_net_1/model_2/rescaling_3/Add:0\", shape=(None, 3), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(5,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1,\n",
    "    callbacks=[coco_metrics_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSNh4V_JMlFy"
   },
   "source": [
    "## Inference and plotting results\n",
    "\n",
    "KerasCV makes object detection inference simple.  `model.predict(images)`\n",
    "returns a tensor of bounding boxes.  By default, `YOLOV8Detector.predict()`\n",
    "will perform a non max suppression operation for you.\n",
    "\n",
    "In this section, we will use a `keras_cv` provided preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bv9Zz6P1MlFy"
   },
   "outputs": [],
   "source": [
    "model = keras_cv.models.YOLOV8Detector.from_preset(\n",
    "    \"yolo_v8_m_pascalvoc\", bounding_box_format=\"xywh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u74w_-yaMlFy"
   },
   "source": [
    "Next, for convenience we construct a dataset with larger batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAE4vaYkMlFz"
   },
   "outputs": [],
   "source": [
    "visualization_ds = eval_ds.unbatch()\n",
    "visualization_ds = visualization_ds.ragged_batch(16)\n",
    "visualization_ds = visualization_ds.shuffle(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpLGfbwWMlFz"
   },
   "source": [
    "You may need to configure your NonMaxSuppression operation to achieve\n",
    "visually appealing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS3B1o3MMlFz"
   },
   "outputs": [],
   "source": [
    "model.prediction_decoder = keras_cv.layers.NonMaxSuppression(\n",
    "    bounding_box_format=\"xywh\",\n",
    "    from_logits=True,\n",
    "    iou_threshold=0.5,\n",
    "    confidence_threshold=0.75,\n",
    ")\n",
    "\n",
    "visualize_detections(model, dataset=visualization_ds, bounding_box_format=\"xywh\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "object_detection_keras_cv",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
