{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5905feac-563e-4b2f-80df-7be52dfdc5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from retinanet import swap_xy, convert_to_xywh, convert_to_corners, compute_iou, visualize_detections, random_flip_horizontal, resize_and_pad_image, preprocess_data, get_backbone, build_head, prepare_image, LabelEncoder, RetinaNetLoss, RetinaNet\n",
    "\n",
    "\"\"\"\n",
    "## Setting up training parameters\n",
    "\"\"\"\n",
    "\n",
    "model_dir = \"retinanet/\"\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "num_classes = 80\n",
    "batch_size = 2\n",
    "\n",
    "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=learning_rate_boundaries, values=learning_rates\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "## Initializing and compiling model\n",
    "\"\"\"\n",
    "\n",
    "resnet50_backbone = get_backbone()\n",
    "loss_fn = RetinaNetLoss(num_classes)\n",
    "model = RetinaNet(num_classes, resnet50_backbone)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
    "model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "\"\"\"\n",
    "## Setting up callbacks\n",
    "\"\"\"\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "## Load the COCO2017 dataset using TensorFlow Datasets\n",
    "\"\"\"\n",
    "\n",
    "#  set `data_dir=None` to load the complete dataset\n",
    "\n",
    "# (train_dataset, val_dataset), dataset_info = tfds.load(\n",
    "#     \"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir=\"data\"\n",
    "# )\n",
    "\n",
    "\"\"\"\n",
    "## Setting up a `tf.data` pipeline\n",
    "\n",
    "To ensure that the model is fed with data efficiently we will be using\n",
    "`tf.data` API to create our input pipeline. The input pipeline\n",
    "consists for the following major processing steps:\n",
    "\n",
    "- Apply the preprocessing function to the samples\n",
    "- Create batches with fixed batch size. Since images in the batch can\n",
    "have different dimensions, and can also have different number of\n",
    "objects, we use `padded_batch` to the add the necessary padding to create\n",
    "rectangular tensors\n",
    "- Create targets for each sample in the batch using `LabelEncoder`\n",
    "\"\"\"\n",
    "\n",
    "# autotune = tf.data.AUTOTUNE\n",
    "# train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "# train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "# train_dataset = train_dataset.padded_batch(\n",
    "#     batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
    "# )\n",
    "# train_dataset = train_dataset.map(\n",
    "#     label_encoder.encode_batch, num_parallel_calls=autotune\n",
    "# )\n",
    "# train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "# train_dataset = train_dataset.prefetch(autotune)\n",
    "\n",
    "# val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "# val_dataset = val_dataset.padded_batch(\n",
    "#     batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
    "# )\n",
    "# val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "# val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "# val_dataset = val_dataset.prefetch(autotune)\n",
    "\n",
    "\"\"\"\n",
    "## Training the model\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment the following lines, when training on full dataset\n",
    "# train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // batch_size\n",
    "# val_steps_per_epoch = \\\n",
    "#     dataset_info.splits[\"validation\"].num_examples // batch_size\n",
    "\n",
    "# train_steps = 4 * 100000\n",
    "# epochs = train_steps // train_steps_per_epoch\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "# Running 100 training and 50 validation steps,\n",
    "# remove `.take` when training on the full dataset\n",
    "\n",
    "# model.fit(\n",
    "#     train_dataset.take(100),\n",
    "#     validation_data=val_dataset.take(50),\n",
    "#     epochs=epochs,\n",
    "#     callbacks=callbacks_list,\n",
    "#     verbose=1,\n",
    "# )\n",
    "\n",
    "\"\"\"\n",
    "## Loading weights\n",
    "\"\"\"\n",
    "\n",
    "# Change this to `model_dir` when not using the downloaded weights\n",
    "weights_dir = \"model_dir\"\n",
    "\n",
    "# latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
    "# model.load_weights(latest_checkpoint)\n",
    "\n",
    "\"\"\"\n",
    "## Building inference model\n",
    "\"\"\"\n",
    "\n",
    "# image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
    "# predictions = model(image, training=False)\n",
    "# detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n",
    "# inference_model = tf.keras.Model(inputs=image, outputs=detections)\n",
    "\n",
    "\"\"\"\n",
    "## Generating detections\n",
    "\"\"\"\n",
    "\n",
    "# val_dataset = tfds.load(\"coco/2017\", split=\"validation\", data_dir=\"data\")\n",
    "# int2str = dataset_info.features[\"objects\"][\"label\"].int2str\n",
    "\n",
    "# for sample in val_dataset.take(2):\n",
    "#     image = tf.cast(sample[\"image\"], dtype=tf.float32)\n",
    "#     input_image, ratio = prepare_image(image)\n",
    "#     detections = inference_model.predict(input_image)\n",
    "#     num_detections = detections.valid_detections[0]\n",
    "#     class_names = [\n",
    "#         int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
    "#     ]\n",
    "#     visualize_detections(\n",
    "#         image,\n",
    "#         detections.nmsed_boxes[0][:num_detections] / ratio,\n",
    "#         class_names,\n",
    "#         detections.nmsed_scores[0][:num_detections],\n",
    "#     )\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa40b9-30e9-4c89-a394-8354e5281e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
