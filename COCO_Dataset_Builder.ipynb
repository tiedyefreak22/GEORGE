{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada490e2-d38b-4577-8b3e-1184bd490e29",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# Gradient-Effected Object Recognition Gauge for hive Entrances (GEORGE)\n",
    "Machine-learning-powered honeybee hive-mounted pollen, varroa, and wasp counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73e41d-514c-4d08-8e54-95fdc852bf91",
   "metadata": {},
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a796c8d8-7f9b-4f19-97ce-2325383b1235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "num_images = 1 # Number of images to create for the dataset\n",
    "num_bees = 4 # Maximum number of bees to include per image\n",
    "IMAGE_WIDTH = 25\n",
    "IMAGE_HEIGHT = 50\n",
    "EX_SUB = 1\n",
    "VAL_SIZE = 0.1\n",
    "FILL_IMG = 0 # need to change; inpaint only works with 3-channel images\n",
    "# num_sets = 1\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9084855-f6e4-42a8-9b53-7be3d57cec49",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df31cb0e-44f6-4a1d-8f17-1f826a6b1078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khard\\.conda\\envs\\tf\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.3 when it was built against 1.14.2, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow_datasets as tfds\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import GEORGE_Library as GEORGE\n",
    "import sys\n",
    "sys.path.append(\"BestBGRemove\")\n",
    "from BestBGRemove import do_image\n",
    "sys.path.append(\"Detection_to_XML\")\n",
    "from Detection_to_XML import CreateXMLfile, xml_to_csv\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e171a2-0c00-4ced-87b5-edfaa37d02a0",
   "metadata": {},
   "source": [
    "## YOLO Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41dbc62d-e3c9-46fa-9859-03d6bb3a8442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "{\n",
    "\"info\": info, \"images\": [image], \"annotations\": [annotation], \"licenses\": [license],\n",
    "}\n",
    " \n",
    "info{\n",
    "\"year\": int, \"version\": str, \"description\": str, \"contributor\": str, \"url\": str, \"date_created\": datetime,\n",
    "}\n",
    " \n",
    "image{\n",
    "\"id\": int, \"width\": int, \"height\": int, \"file_name\": str, \"license\": int, \"flickr_url\": str, \"coco_url\": str, \"date_captured\": datetime,\n",
    "}\n",
    " \n",
    "license{\n",
    "\"id\": int, \"name\": str, \"url\": str,\n",
    "}\n",
    " \n",
    "annotation{\n",
    "\"id\": int, \"image_id\": int, \"category_id\": int, \"segmentation\": RLE or [polygon], \"area\": float, \"bbox\": [x,y,width,height], \"iscrowd\": 0 or 1,\n",
    "}\n",
    " \n",
    "categories[{\n",
    "\"id\": int, \"name\": str, \"supercategory\": str,\n",
    "}]\n",
    "'''\n",
    "\n",
    "class img:\n",
    "    def __init__(self, img_id: int, width: int, height: int, file_name: str, license: int, flickr_url: str, coco_url: str, date_captured: str):\n",
    "        self.img_id = img_id\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.file_name = file_name\n",
    "        self.license = license\n",
    "        self.flickr_url = flickr_url\n",
    "        self.coco_url = coco_url\n",
    "        self.date_captured = date_captured\n",
    "    def toJSON(self):\n",
    "        return json.dumps(\n",
    "            self,\n",
    "            default=lambda o: o.__dict__, \n",
    "            sort_keys=False,\n",
    "            indent=4)\n",
    " \n",
    "class license:\n",
    "    def __init__(self, lic_id: int, name: str, url: str):\n",
    "        self.lic_id = lic_id\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "    def toJSON(self):\n",
    "        return json.dumps(\n",
    "            self,\n",
    "            default=lambda o: o.__dict__, \n",
    "            sort_keys=False,\n",
    "            indent=4)\n",
    " \n",
    "class bbox:\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    " \n",
    "class annotation:\n",
    "    def __init__(self, ann_id: int, image_id: int, category_id: int, segmentation, area: float, bbox: bbox, iscrowd: bool):\n",
    "        self.ann_id = ann_id\n",
    "        self.image_id = image_id\n",
    "        self.category_id: category_id\n",
    "        self.segmentation = segmentation\n",
    "        self.area = area\n",
    "        self.bbox = list(vars(bbox).values())\n",
    "        self.iscrowd = iscrowd\n",
    "    def toJSON(self):\n",
    "        return json.dumps(\n",
    "            self,\n",
    "            default=lambda o: o.__dict__, \n",
    "            sort_keys=False,\n",
    "            indent=4)\n",
    " \n",
    "class category:\n",
    "    def __init__(self, cat_id: int, name: str, supercategory: str):\n",
    "        self.cat_id = cat_id\n",
    "        self.name = name\n",
    "        self.supercategory = supercategory\n",
    " \n",
    "info = {\n",
    "        \"year\": int,\n",
    "        \"version\": str,\n",
    "        \"description\": str,\n",
    "        \"contributor\": str,\n",
    "        \"url\": str,\n",
    "        \"date_created\": datetime,\n",
    "        }\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82719d02-b734-484a-9e2e-c2f7e57d1ff8",
   "metadata": {},
   "source": [
    "## Feeder Image Array Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20fa0722-512e-4db9-96e9-cd12854854e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_augmented_fp = \"Dataset/TrainAugmented\"\n",
    "val_augmented_fp = \"Dataset/ValAugmented\"\n",
    "pi_fp = 'Pi_Images'\n",
    "\n",
    "# try:\n",
    "#     files = os.listdir(train_augmented_fp)\n",
    "#     for file in files:\n",
    "#         file_path = os.path.join(train_augmented_fp, file)\n",
    "#         if os.path.isfile(file_path):\n",
    "#             os.remove(file_path)\n",
    "#     files = os.listdir(val_augmented_fp)\n",
    "#     for file in files:\n",
    "#         file_path = os.path.join(val_augmented_fp, file)\n",
    "#         if os.path.isfile(file_path):\n",
    "#             os.remove(file_path)\n",
    "# except OSError:\n",
    "#     print(\"Error\")\n",
    "\n",
    "dataset_paths = {\n",
    "                 'PD': {'none': {'path': 'Dataset/PollenDataset/None', 'label': [0, 0, 0]},\n",
    "                        'pollen': {'path': 'Dataset/PollenDataset/Pollen', 'label': [1, 0, 0]}},\n",
    "                 'BA': {'none': {'path': 'Dataset/BeeAlarmed/None', 'label': [0, 0, 0]},\n",
    "                        'pollen': {'path': 'Dataset/BeeAlarmed/Pollen', 'label': [1, 0, 0]},\n",
    "                        'varroa': {'path': 'Dataset/BeeAlarmed/Varroa', 'label': [0, 1, 0]},\n",
    "                        'wasps': {'path': 'Dataset/BeeAlarmed/Wasps', 'label': [0, 0, 1]}},\n",
    "                 'YM': {'none': {'path': 'Dataset/YangModel/None', 'label': [0, 0, 0]},\n",
    "                        'pollen': {'path': 'Dataset/YangModel/Pollen', 'label': [1, 0, 0]},\n",
    "                        'varroa': {'path': 'Dataset/YangModel/Varroa', 'label': [0, 1, 0]}},\n",
    "                }\n",
    "                 # 'USU': {'none': {'path': 'Dataset/USU/None', 'label': [0, 0, 0]},\n",
    "                 #         'pollen': {'path': 'Dataset/USU/Pollen', 'label': [1, 0, 0]},\n",
    "                 #         'varroa': {'path': 'Dataset/USU/Varroa', 'label': [0, 1, 0]}}}\n",
    "\n",
    "file_list = []\n",
    "for dataset_path in dataset_paths:\n",
    "    for dataset in dataset_paths[dataset_path]:\n",
    "        [file_list.append(i) for i in glob.glob(dataset_paths[dataset_path][dataset]['path'] + '/*')]\n",
    "\n",
    "pi_list = []\n",
    "[pi_list.append(i) for i in glob.glob(pi_fp + '/*')]\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5104a9f-27a4-4090-9adb-1b23e2fa897f",
   "metadata": {},
   "source": [
    "## Dataset Creation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "364d14c6-f6a6-45bb-b498-8ab13b23c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: COCO Dataset IDs start with 1\n",
    "\n",
    "images = []\n",
    "annotations = []\n",
    "licenses = []\n",
    "\n",
    "categories = [category(1, \"None\", None), category(2, \"Pollen\", None), category(3, \"Varroa\", None), category(4, \"Wasps\", None)]\n",
    "\n",
    "# License creation \"loop\"\n",
    "License_Id = 1\n",
    "Name = 'Attribution-NonCommercial-ShareAlike 4.0 International'\n",
    "Url = 'https://creativecommons.org/licenses/by-nc-sa/4.0/'\n",
    "licenses.append(license(License_Id, Name, Url))\n",
    "\n",
    "img_width = 640\n",
    "img_height = 640\n",
    "License = 1\n",
    "Flickr_url = None\n",
    "Coco_url = None\n",
    "\n",
    "for image_id in range(num_images):\n",
    "    BG_img = Image.open(random.choice(pi_list)).convert('RGB')\n",
    "    BG_width, BG_height = BG_img.size\n",
    "    i = random.randint(0,2)\n",
    "    left = 640 * i\n",
    "    bottom =  int(BG_height - ((BG_height - 640) / 2))\n",
    "    right = 640 * (i + 1)\n",
    "    top = int((BG_height - 640) / 2)\n",
    "    BG_img = BG_img.crop((left, top, right, bottom))\n",
    "    \n",
    "    Id = image_id + 1\n",
    "    File_name = Id\n",
    "    Date_captured = str(datetime.now())\n",
    "\n",
    "    # Feeder image usage sub-loop (annotation sub-loop)\n",
    "    for bee in range(random.randint(1, num_bees)): # number of bees to include in image\n",
    "        Annotation_Id = bee\n",
    "        Image_id = Id\n",
    "\n",
    "        current_bee = os.path.normpath(random.choice(file_list)) # Grab a random bee from feeder images\n",
    "        category_name = os.path.dirname(current_bee).split('\\\\')[-1] # Get category of feeder image from containing directory\n",
    "        Category_id = [i.cat_id for i in categories if i.name == category_name][0]\n",
    "\n",
    "        image = Image.open(current_bee).convert('RGB')\n",
    "        new_image = image\n",
    "        if max(image.size) < min(IMAGE_WIDTH, IMAGE_HEIGHT): # Resize feeder image if required\n",
    "            ind = (image.size).index(max(image.size))\n",
    "            size = np.zeros(2)\n",
    "            size[ind] = min(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "            size[1 - ind] = (size[ind] / image.size[ind]) * image.size[1 - ind]\n",
    "            new_image = image.resize((round(size[0]), round(size[1])))\n",
    "        elif min(image.size) > max(IMAGE_WIDTH, IMAGE_HEIGHT):\n",
    "            ind = (image.size).index(min(image.size))\n",
    "            size = np.zeros(2)\n",
    "            size[ind] = max(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "            size[1 - ind] = (size[ind] / image.size[ind]) * image.size[1 - ind]\n",
    "            new_image = image.resize((round(size[0]), round(size[1])))\n",
    "        if np.array(new_image).max() <= 1: # Normalize feeder image if required\n",
    "            new_image = np.array(new_image) * 255\n",
    "        new_image = GEORGE.automatic_brightness_and_contrast(np.array(new_image).astype('uint8'))\n",
    "        new_image = GEORGE.zoom_image(new_image)\n",
    "        if EX_SUB:\n",
    "            if current_bee.split(\"\\\\\")[1] == \"PollenDataset\":\n",
    "                starting_params = [159, 217, 81, [], 84, 191, 1, 9, 5.706, 3]\n",
    "                new_image, _ = do_image(np.array(new_image), *starting_params)\n",
    "                #Image.fromarray(img1).save(tmp_fp + current_bee.split(\"\\\\\")[0] + \"\\\\\" + current_bee.split(\"\\\\\")[-1].split(\".\")[0] + \".png\")\n",
    "            elif current_bee.split(\"\\\\\")[1] == \"BeeAlarmed\":\n",
    "                starting_params = [71, 203, 177, [], 134, 209, 1, 9, 3.204, 9]\n",
    "                new_image, _ = do_image(np.array(new_image), *starting_params)\n",
    "                #Image.fromarray(img1).save(tmp_fp + current_bee.split(\"\\\\\")[0] + \"\\\\\" + current_bee.split(\"\\\\\")[-1].split(\".\")[0] + \".png\")\n",
    "            elif current_bee.split(\"\\\\\")[1] == \"YangModel\":\n",
    "                pass\n",
    "        BG_img, new_coords = GEORGE.process_image(np.array(new_image).astype('uint8'), BG_img)\n",
    "        new_coords = new_coords * 640\n",
    "        str_name = []\n",
    "        for coord in new_coords:\n",
    "            str_name.append(str(coord).split('.')[1])\n",
    "\n",
    "        Segmentation = None\n",
    "        X = new_coords[1]\n",
    "        Y = new_coords[0]\n",
    "        Width = new_coords[3] - X\n",
    "        Height = new_coords[2] - Y\n",
    "        Area = Width * Height\n",
    "        Bbox = bbox(X, Y, Width, Height)\n",
    "        Iscrowd = 0 # Pretty sure this is perpetually zero for this dataset\n",
    "\n",
    "        annotations.append(annotation(Annotation_Id, Image_id, Category_id, Segmentation, Area, Bbox, Iscrowd))\n",
    "    Image.fromarray(np.array(BG_img).astype('uint8')).save(\"Dataset/Custom_Dataset/\" + str(File_name) + \".png\",\"PNG\")\n",
    "    images.append(img(Id, img_width, img_height, File_name, License, Flickr_url, Coco_url, Date_captured))\n",
    "\n",
    "info = {\n",
    "        \"year\": 2024,\n",
    "        \"version\": 1.0,\n",
    "        \"description\": \"GEORGE Custom Dataset\",\n",
    "        \"contributor\": \"Kevin Hardin\",\n",
    "        \"url\": \"https://www.kaggle.com/datasets/kevinhardin/george-augmented-dataset\",\n",
    "        \"date_created\": str(datetime.now()),\n",
    "        }\n",
    "\n",
    "with open(\"custom_bee_dataset.json\", \"w\") as outfile:\n",
    "    json.dump({\"info\": info, \"images\": [json.loads(i.toJSON()) for i in images], \"annotations\": [json.loads(i.toJSON()) for i in annotations], \"licenses\": [json.loads(i.toJSON()) for i in licenses]}, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce2259-2523-41bc-b19a-cad4fb4c87e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
