{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb5d7f0-7d01-424e-ba90-04d7a22eb7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\\n\"info\": info, \"images\": [image], \"annotations\": [annotation], \"licenses\": [license],\\n}\\n \\ninfo{\\n\"year\": int, \"version\": str, \"description\": str, \"contributor\": str, \"url\": str, \"date_created\": datetime,\\n}\\n \\nimage{\\n\"id\": int, \"width\": int, \"height\": int, \"file_name\": str, \"license\": int, \"flickr_url\": str, \"coco_url\": str, \"date_captured\": datetime,\\n}\\n \\nlicense{\\n\"id\": int, \"name\": str, \"url\": str,\\n}\\n \\nannotation{\\n\"id\": int, \"image_id\": int, \"category_id\": int, \"segmentation\": RLE or [polygon], \"area\": float, \"bbox\": [x,y,width,height], \"iscrowd\": 0 or 1,\\n}\\n \\ncategories[{\\n\"id\": int, \"name\": str, \"supercategory\": str,\\n}]\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "{\n",
    "\"info\": info, \"images\": [image], \"annotations\": [annotation], \"licenses\": [license],\n",
    "}\n",
    " \n",
    "info{\n",
    "\"year\": int, \"version\": str, \"description\": str, \"contributor\": str, \"url\": str, \"date_created\": datetime,\n",
    "}\n",
    " \n",
    "image{\n",
    "\"id\": int, \"width\": int, \"height\": int, \"file_name\": str, \"license\": int, \"flickr_url\": str, \"coco_url\": str, \"date_captured\": datetime,\n",
    "}\n",
    " \n",
    "license{\n",
    "\"id\": int, \"name\": str, \"url\": str,\n",
    "}\n",
    " \n",
    "annotation{\n",
    "\"id\": int, \"image_id\": int, \"category_id\": int, \"segmentation\": RLE or [polygon], \"area\": float, \"bbox\": [x,y,width,height], \"iscrowd\": 0 or 1,\n",
    "}\n",
    " \n",
    "categories[{\n",
    "\"id\": int, \"name\": str, \"supercategory\": str,\n",
    "}]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41dbc62d-e3c9-46fa-9859-03d6bb3a8442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 'None', None]\n",
      "[1, 'Pollen', None]\n",
      "[2, 'Cooling', None]\n",
      "[3, 'Wasp', None]\n",
      "['input', 'output']\n",
      "(300, 150, 3) {'cooling_output': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'pollen_output': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'varroa_output': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'wasps_output': <tf.Tensor: shape=(), dtype=float64, numpy=1.0>}\n",
      "{'x': 3, 'y': 2, 'width': 6, 'height': 5}\n",
      "[3, 2, 6, 5]\n",
      "{'id': 0, 'image_id': 0, 'segmentation': None, 'area': 100, 'bbox': [3, 2, 6, 5], 'iscrowd': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 21:08:56.553448: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow_datasets as tfds\n",
    "import json\n",
    " \n",
    "class image:\n",
    "    def __init__(self, id: int, width: int, height: int, file_name: str, license: int, flickr_url: str, coco_url: str, date_captured: datetime):\n",
    "        self.id = id\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.file_name = file_name\n",
    "        self.license = license\n",
    "        self.flickr_url = flickr_url\n",
    "        self.coco_url = coco_url\n",
    "        self.date_captured = date_captured\n",
    " \n",
    "class license:\n",
    "    def __init__(self, id: int, name: str, url: str):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.url = url\n",
    " \n",
    "class bbox:\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    " \n",
    "class annotation:\n",
    "    def __init__(self, id: int, image_id: int, category_id: int, segmentation, area: float, bbox: bbox, iscrowd: bool):\n",
    "        self.id = id\n",
    "        self.image_id = image_id\n",
    "        self.category_id: category_id\n",
    "        self.segmentation = segmentation\n",
    "        self.area = area\n",
    "        self.bbox = list(vars(bbox).values())\n",
    "        self.iscrowd = iscrowd\n",
    " \n",
    "class category:\n",
    "    def __init__(self, id: int, name: str, supercategory: str):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.supercategory = supercategory\n",
    " \n",
    "info = {\n",
    "        \"year\": int,\n",
    "        \"version\": str,\n",
    "        \"description\": str,\n",
    "        \"contributor\": str,\n",
    "        \"url\": str,\n",
    "        \"date_created\": datetime,\n",
    "        }\n",
    " \n",
    "images = []\n",
    "annotations = []\n",
    "licenses = []\n",
    "categories = [category(0, \"None\", None), category(1, \"Pollen\", None), category(2, \"Cooling\", None), category(3, \"Wasp\", None)]\n",
    "[print(list(vars(category).values())) for category in categories]\n",
    "\n",
    "ds = tfds.load('bee_dataset', split='train')\n",
    "ds = ds.take(1)  # Only take a single example\n",
    "\n",
    "for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    print(list(example.keys()))\n",
    "    image = example[\"input\"]\n",
    "    label = example[\"output\"]\n",
    "    print(image.shape, label)\n",
    "\n",
    "# Id =\n",
    "# Width =\n",
    "# Height =\n",
    "# File_name =\n",
    "# License =\n",
    "Flickr_url = None\n",
    "Coco_url = None\n",
    "Date_captured = datetime.now()\n",
    " \n",
    "# images.append(image(Id, Width, Height, File_name, License, Flickr_url, Coco_url, Date_captured))\n",
    " \n",
    "# Annotation_Id =\n",
    "# Image_id =\n",
    "# Category_id =\n",
    "# Segmentation = None\n",
    "# X =\n",
    "# Y =\n",
    "# Width =\n",
    "# Height =\n",
    "# Area = Width * Height\n",
    "# Bbox = bbox(X, Y, Width, Height)\n",
    "# Iscrowd =\n",
    " \n",
    "# annotations.append(annotation(Annotation_Id, Image_id, Category_id, Segmentation, Area, Bbox, Iscrowd))\n",
    " \n",
    "# License_Id =\n",
    "# Name =\n",
    "# Url =\n",
    " \n",
    "# licenses.append(license(License_Id, Name, Url))\n",
    " \n",
    "# with open(\"custom_bee_dataset.json\", \"w\") as outfile:\n",
    "#     json.dump({\"info\": info, \"images\": images, \"annotations\": annotations, \"licenses\": licenses}, outfile, indent = 4)\n",
    " \n",
    "Bbox = bbox(3, 2, 6, 5)\n",
    "print(vars(Bbox))\n",
    "print(list(vars(Bbox).values()))\n",
    " \n",
    "Annotation = annotation(0, 0, 0, None, 100, Bbox, True)\n",
    "print(vars(Annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa0722-512e-4db9-96e9-cd12854854e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
