{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada490e2-d38b-4577-8b3e-1184bd490e29",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# Gradient-Effected Object Recognition Gauge for hive Entrances (GEORGE)\n",
    "Machine-learning-powered honeybee hive-mounted pollen, varroa, and wasp counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73e41d-514c-4d08-8e54-95fdc852bf91",
   "metadata": {},
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a796c8d8-7f9b-4f19-97ce-2325383b1235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "num_train = 100000 # Number of images to create for the dataset\n",
    "num_val = 15000\n",
    "num_test = 15000\n",
    "num_bees = 15 # Maximum number of bees to include per image\n",
    "EX_SUB = 1\n",
    "version = 1.2\n",
    "\n",
    "overwrite_feeder = 0\n",
    "overwrite_BG = 0\n",
    "overwrite_train = 0\n",
    "overwrite_validation = 0\n",
    "overwrite_test = 0\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9084855-f6e4-42a8-9b53-7be3d57cec49",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df31cb0e-44f6-4a1d-8f17-1f826a6b1078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from GEORGE_Library import *\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82719d02-b734-484a-9e2e-c2f7e57d1ff8",
   "metadata": {},
   "source": [
    "## Feeder Image Array Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20fa0722-512e-4db9-96e9-cd12854854e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_augmented_fp = \"Dataset/Custom_Dataset/Train\"\n",
    "val_augmented_fp = \"Dataset/Custom_Dataset/Validation\"\n",
    "test_augmented_fp = \"Dataset/Custom_Dataset/Test\"\n",
    "feeder_image_fp = \"Dataset/Custom_Dataset/FeederImages\"\n",
    "processed_bg_image_fp = \"Dataset/Custom_Dataset/ProcessedBGImages\"\n",
    "pi_fp = 'Raspberry_Pi/Images/Training_Bkgnd_Imgs'\n",
    "\n",
    "if overwrite_feeder:    \n",
    "    try:\n",
    "        files = os.listdir(feeder_image_fp)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(feeder_image_fp, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "if overwrite_BG:    \n",
    "    try:\n",
    "        files = os.listdir(processed_bg_image_fp)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(processed_bg_image_fp, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "dataset_paths = {\n",
    "                 'PD': {'none': {'path': 'Dataset/PollenDataset/Regular', 'label': [0, 0, 0]},\n",
    "                        'pollen': {'path': 'Dataset/PollenDataset/Pollen', 'label': [1, 0, 0]}},\n",
    "                 'BA': {'none': {'path': 'Dataset/BeeAlarmed/Regular', 'label': [0, 0, 0]},\n",
    "                        'pollen': {'path': 'Dataset/BeeAlarmed/Pollen', 'label': [1, 0, 0]},\n",
    "                        'varroa': {'path': 'Dataset/BeeAlarmed/Varroa', 'label': [0, 1, 0]},\n",
    "                        'wasps': {'path': 'Dataset/BeeAlarmed/Wasps', 'label': [0, 0, 1]}},\n",
    "                }\n",
    "\n",
    "file_list = []\n",
    "for dataset_path in dataset_paths:\n",
    "    for dataset in dataset_paths[dataset_path]:\n",
    "        [file_list.append(i) for i in glob.glob(dataset_paths[dataset_path][dataset]['path'] + '/*')]\n",
    "\n",
    "pi_list = []\n",
    "[pi_list.append(i) for i in glob.glob(pi_fp + '/*')]\n",
    "\n",
    "categories = [category(1, \"Regular\", None), category(2, \"Pollen\", None), category(3, \"Varroa\", None), category(4, \"Wasps\", None)]\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f61b20-a5e8-43f4-91cb-9d85b093cc24",
   "metadata": {},
   "source": [
    "## Feeder Image Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b09ef3-4f45-45d3-8414-1a67bb4f4656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 7187/7187 [00:00<00:00, 18644.15image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "magic_wand_threshold = 100\n",
    "\n",
    "existing_files = []\n",
    "files = os.listdir(feeder_image_fp)\n",
    "for file in files:\n",
    "    file_path = os.path.join(feeder_image_fp, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        existing_files.append(file_path)\n",
    "\n",
    "with tqdm(total = len(file_list), unit=\"image\") as pbar:\n",
    "    for image_id, current_image in enumerate(file_list):        \n",
    "        Id = image_id + 1\n",
    "        Image_id = Id\n",
    "\n",
    "        current_bee = os.path.normpath(file_list[image_id]) # Grab the next feeder image\n",
    "        category_name = os.path.dirname(current_bee).split('\\\\')[-1] # Get category of feeder image from containing directory\n",
    "        Category_id = [i.id for i in categories if i.name == category_name][0]\n",
    "        File_name = str(Id) + \"_\" + str(Category_id) + '.png'\n",
    "\n",
    "        feeder_image_save_fp = feeder_image_fp + '\\\\' + str(File_name)\n",
    "        if not feeder_image_save_fp in existing_files:\n",
    "            image = Image.open(current_image).convert('RGB')\n",
    "            new_image = deepcopy(image)\n",
    "    \n",
    "            if max(image.size) < min(IMAGE_WIDTH, IMAGE_HEIGHT): # Resize feeder image if required\n",
    "                ind = np.argmax(image.size)\n",
    "                size = np.zeros(2)\n",
    "                size[ind] = min(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "                size[1 - ind] = (size[ind] / image.size[ind]) * image.size[1 - ind]\n",
    "                new_image = image.resize((round(size[0]), round(size[1])))\n",
    "            elif min(image.size) > max(IMAGE_WIDTH, IMAGE_HEIGHT):\n",
    "                ind = np.argmin(image.size)\n",
    "                size = np.zeros(2)\n",
    "                size[ind] = max(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "                size[1 - ind] = (size[ind] / image.size[ind]) * image.size[1 - ind]\n",
    "                new_image = image.resize((round(size[0]), round(size[1])))\n",
    "            if np.array(new_image).max() <= 1: # Normalize feeder image if required\n",
    "                new_image = np.array(new_image) * 255\n",
    "            new_image = automatic_brightness_and_contrast(np.array(new_image).astype('uint8'))\n",
    "            if EX_SUB:\n",
    "                if current_bee.split(\"\\\\\")[1] == \"PollenDataset\":\n",
    "                    img_mode = get_mode(new_image)\n",
    "                    new_image = cv2.copyMakeBorder(new_image, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value= (int(img_mode[0]), int(img_mode[1]), int(img_mode[2])))\n",
    "                    \n",
    "                    value_mask = np.ones_like(new_image[:, :, 0]) \n",
    "                    for j in range(np.shape(new_image)[1]):\n",
    "                        # get the indicies of all first and last rows and columns, then pass to new magic wand function and \"and\" the masks together\n",
    "                        current_value_mask_top = magic_wand(new_image, (0, j), magic_wand_threshold, contiguous = True)\n",
    "                        current_value_mask_bottom = magic_wand(new_image, (-1, j), magic_wand_threshold, contiguous = True)\n",
    "                        value_mask = cv2.bitwise_and(value_mask, current_value_mask_top)\n",
    "                        value_mask = cv2.bitwise_and(value_mask, current_value_mask_bottom)\n",
    "                    for i in range(np.shape(new_image)[0]):\n",
    "                        current_value_mask_left = magic_wand(new_image, (i, 0), magic_wand_threshold, contiguous = True)\n",
    "                        current_value_mask_right = magic_wand(new_image, (i, -1), magic_wand_threshold, contiguous = True)\n",
    "                        value_mask = cv2.bitwise_and(value_mask, current_value_mask_left)\n",
    "                        value_mask = cv2.bitwise_and(value_mask, current_value_mask_right)\n",
    "                    \n",
    "                    new_image = np.dstack((new_image, value_mask * 255))\n",
    "                    new_image = new_image[1:-1, 1:-1]\n",
    "                elif current_bee.split(\"\\\\\")[1] == \"BeeAlarmed\":\n",
    "                    img_mode = get_mode(new_image)\n",
    "                    new_image = cv2.copyMakeBorder(new_image, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value= (int(img_mode[0]), int(img_mode[1]), int(img_mode[2])))\n",
    "                    \n",
    "                    value_mask = np.ones_like(new_image[:, :, 0]) \n",
    "                    for j in range(np.shape(new_image)[1]):\n",
    "                        # get the indicies of all first and last rows and columns, then pass to new magic wand function and \"and\" the masks together\n",
    "                        current_value_mask_top = magic_wand(new_image, (0, j), magic_wand_threshold, contiguous = True)\n",
    "                        current_value_mask_bottom = magic_wand(new_image, (-1, j), magic_wand_threshold, contiguous = True)\n",
    "                        value_mask = cv2.bitwise_and(value_mask, current_value_mask_top)\n",
    "                        value_mask = cv2.bitwise_and(value_mask, current_value_mask_bottom)\n",
    "                    for i in range(np.shape(new_image)[0]):\n",
    "                        # get the indicies of all first and last rows and columns, then pass to new magic wand function and \"and\" the masks together\n",
    "                        current_value_mask_left = magic_wand(new_image, (i, 0), magic_wand_threshold, contiguous = True)\n",
    "                        current_value_mask_right = magic_wand(new_image, (i, -1), magic_wand_threshold, contiguous = True)\n",
    "                        value_mask = cv2.bitwise_and(value_mask, current_value_mask_left)\n",
    "                        value_mask = cv2.bitwise_and(value_mask, current_value_mask_right)\n",
    "                    \n",
    "                    new_image = np.dstack((new_image, value_mask * 255))\n",
    "                    new_image = new_image[1:-1, 1:-1]\n",
    "            Image.fromarray(np.array(new_image).astype('uint8')).save(feeder_image_save_fp, \"PNG\")\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a730cb4-25c9-4baf-87b1-969cbe525126",
   "metadata": {},
   "source": [
    "## Background Image Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f8700c-7c37-42fa-996b-079deef2ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "img_width = 640\n",
    "img_height = 640\n",
    "\n",
    "bg_list = []\n",
    "[bg_list.append(i) for i in glob.glob(processed_bg_image_fp + '/*')]\n",
    "\n",
    "if overwrite_BG or (len(bg_list) == 0):\n",
    "    with tqdm(total = len(pi_list), unit=\"image\") as pbar:\n",
    "        for idx, image in enumerate(pi_list):\n",
    "            for i in range(2):\n",
    "                BG_img = Image.open(random.choice(pi_list)).convert('RGB')\n",
    "                BG_img = automatic_brightness_and_contrast(np.array(BG_img).astype('uint8'))\n",
    "                BG_height, BG_width, _ = BG_img.shape\n",
    "                left = img_width * i\n",
    "                bottom =  int(BG_height - ((BG_height - img_height) / 2))\n",
    "                right = img_width * (i + 1)\n",
    "                top = int((BG_height - img_height) / 2)\n",
    "                BG_img = Image.fromarray(BG_img).crop((left, top, right, bottom))\n",
    "        \n",
    "                BG_img.save(processed_bg_image_fp + '/' + str(idx) + \"_\" + str(i) + \".png\", \"PNG\")\n",
    "            pbar.update(1)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5104a9f-27a4-4090-9adb-1b23e2fa897f",
   "metadata": {},
   "source": [
    "## Training Dataset Creation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364d14c6-f6a6-45bb-b498-8ab13b23c14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                     | 1517/70053 [14:09<10:39:56,  1.78image/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: COCO Dataset IDs start with 1\n",
    "\n",
    "feeder_image_file_list = []\n",
    "[feeder_image_file_list.append(i) for i in glob.glob(feeder_image_fp + '/*')]\n",
    "\n",
    "bg_list = []\n",
    "[bg_list.append(i) for i in glob.glob(processed_bg_image_fp + '/*')]\n",
    "\n",
    "images = []\n",
    "annotations = []\n",
    "licenses = []\n",
    "\n",
    "# License creation \"loop\"\n",
    "License_Id = 1\n",
    "Name = 'Attribution-NonCommercial-ShareAlike 4.0 International'\n",
    "Url = 'https://creativecommons.org/licenses/by-nc-sa/4.0/'\n",
    "licenses.append(license(License_Id, Name, Url))\n",
    "\n",
    "License = 1\n",
    "Flickr_url = None\n",
    "Coco_url = None\n",
    "# Annotation_Id = 1\n",
    "\n",
    "def dataset_creation_loop(num_images, augmented_fp, overwrite = False):\n",
    "    if overwrite:\n",
    "        try:\n",
    "            files = os.listdir(augmented_fp)\n",
    "            for file in files:\n",
    "                file_path = os.path.join(augmented_fp, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    existing_files = []\n",
    "    files = os.listdir(augmented_fp)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(augmented_fp, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            existing_files.append(file_path)\n",
    "\n",
    "    existing_num_images = len(existing_files)\n",
    "    Annotation_Id = 1\n",
    "    with tqdm(total = num_images - existing_num_images, unit=\"image\") as pbar:\n",
    "        for image_id in range(num_images - existing_num_images):\n",
    "            BG_img = Image.open(random.choice(bg_list)).convert('RGB')\n",
    "            \n",
    "            Id = image_id + 1\n",
    "            File_name = str(Id) + '.png'\n",
    "            Date_captured = str(datetime.now())\n",
    "            \n",
    "            # Feeder image usage sub-loop (annotation sub-loop)\n",
    "            for bee in range(random.randint(1, num_bees)): # number of bees to include in image\n",
    "                Image_id = Id\n",
    "        \n",
    "                current_bee = os.path.normpath(random.choice(feeder_image_file_list)) # Grab a random bee from feeder images\n",
    "                Category_id = int(os.path.basename(current_bee).split('_')[-1].split('.png')[0])\n",
    "                category_name = [i.name for i in categories if i.id == Category_id][0] # Get category of feeder image from containing directory\n",
    "\n",
    "                image = Image.open(current_bee).convert('RGBA')\n",
    "                new_image = deepcopy(image)\n",
    "                new_image = zoom_image(new_image, 0.5) # second argument is zoom factor\n",
    "                new_image = brightness_match(BG_img, new_image)\n",
    "                new_image = random_motion_blur(new_image)\n",
    "                BG_img, new_coords = process_image(np.array(new_image).astype('uint8'), BG_img)\n",
    "                new_coords = new_coords * 640 # ideally need to put this in terms of desired img size\n",
    "                str_name = []\n",
    "                for coord in new_coords:\n",
    "                    str_name.append(str(coord).split('.')[1])\n",
    "        \n",
    "                Segmentation = None\n",
    "                X = new_coords[1]\n",
    "                Y = new_coords[0]\n",
    "                Width = new_coords[3] - X\n",
    "                Height = new_coords[2] - Y\n",
    "                Area = Width * Height\n",
    "                Bbox = bbox(X, Y, Width, Height)\n",
    "                Iscrowd = 0 # Pretty sure this is perpetually zero for this dataset\n",
    "                \n",
    "                annotations.append(annotation(Annotation_Id, Image_id, Category_id, Segmentation, Area, Bbox, Iscrowd))\n",
    "                Annotation_Id = Annotation_Id + 1\n",
    "\n",
    "            BG_img = automatic_brightness_and_contrast(np.array(BG_img).astype('uint8'))\n",
    "            BG_img = automatic_saturation(BG_img)\n",
    "            # BG_img = automatic_tint(BG_img)\n",
    "            Image.fromarray(BG_img).save(augmented_fp + '/' + str(File_name),\"PNG\")\n",
    "            images.append(img(Id, img_width, img_height, File_name, License, Flickr_url, Coco_url, Date_captured))\n",
    "            pbar.update(1)\n",
    "\n",
    "dataset_creation_loop(num_train, train_augmented_fp, overwrite = overwrite_train)\n",
    "\n",
    "if overwrite_train:\n",
    "    info = {\n",
    "            \"year\": str(datetime.now().year),\n",
    "            \"version\": version,\n",
    "            \"description\": \"GEORGE Custom Training Dataset\",\n",
    "            \"contributor\": \"Kevin Hardin\",\n",
    "            \"url\": \"https://www.kaggle.com/datasets/kevinhardin/george-augmented-dataset\",\n",
    "            \"date_created\": str(datetime.now()),\n",
    "            }\n",
    "    \n",
    "    with open(train_augmented_fp + \"/custom_bee_dataset.json\", \"w\") as outfile:\n",
    "        json.dump({\"info\": info, \"images\": [json.loads(i.toJSON()) for i in images], \"annotations\": [json.loads(i.toJSON()) for i in annotations], \"licenses\": [json.loads(i.toJSON()) for i in licenses], \"categories\": [json.loads(i.toJSON()) for i in categories]}, outfile, indent = 4)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263bb22-5a4f-4684-b76c-28a641d75128",
   "metadata": {},
   "source": [
    "## Validation Dataset Creation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f6e38-ebf5-4c80-b18f-7d5afa4178fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "annotations = []\n",
    "licenses = []\n",
    "\n",
    "dataset_creation_loop(num_val, val_augmented_fp, overwrite = overwrite_validation)\n",
    "\n",
    "if overwrite_validation:\n",
    "    info = {\n",
    "            \"year\": str(datetime.now().year),\n",
    "            \"version\": version,\n",
    "            \"description\": \"GEORGE Custom Validation Dataset\",\n",
    "            \"contributor\": \"Kevin Hardin\",\n",
    "            \"url\": \"https://www.kaggle.com/datasets/kevinhardin/george-augmented-dataset\",\n",
    "            \"date_created\": str(datetime.now()),\n",
    "            }\n",
    "    \n",
    "    with open(val_augmented_fp + \"/custom_bee_dataset.json\", \"w\") as outfile:\n",
    "        json.dump({\"info\": info, \"images\": [json.loads(i.toJSON()) for i in images], \"annotations\": [json.loads(i.toJSON()) for i in annotations], \"licenses\": [json.loads(i.toJSON()) for i in licenses], \"categories\": [json.loads(i.toJSON()) for i in categories]}, outfile, indent = 4)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99bbbc0-1170-4c51-9dd3-1c76ac804148",
   "metadata": {},
   "source": [
    "## Test Dataset Creation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd22e8-cb28-4966-8ad9-aad935c082b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "annotations = []\n",
    "licenses = []\n",
    "\n",
    "dataset_creation_loop(num_test, test_augmented_fp, overwrite = overwrite_test)\n",
    "\n",
    "if overwrite_test:\n",
    "    info = {\n",
    "            \"year\": str(datetime.now().year),\n",
    "            \"version\": version,\n",
    "            \"description\": \"GEORGE Custom Test Dataset\",\n",
    "            \"contributor\": \"Kevin Hardin\",\n",
    "            \"url\": \"https://www.kaggle.com/datasets/kevinhardin/george-augmented-dataset\",\n",
    "            \"date_created\": str(datetime.now()),\n",
    "            }\n",
    "    \n",
    "    with open(test_augmented_fp + \"/custom_bee_dataset.json\", \"w\") as outfile:\n",
    "        json.dump({\"info\": info, \"images\": [json.loads(i.toJSON()) for i in images], \"annotations\": [json.loads(i.toJSON()) for i in annotations], \"licenses\": [json.loads(i.toJSON()) for i in licenses], \"categories\": [json.loads(i.toJSON()) for i in categories]}, outfile, indent = 4)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08faeca-720d-4733-9d3c-3ea8e45b29bd",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce2259-2523-41bc-b19a-cad4fb4c87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_annotations_file = train_augmented_fp + \"/custom_bee_dataset.json\"\n",
    "with open(coco_annotations_file, 'r') as f:\n",
    "    print(json.load(f))\n",
    "coco = COCOParser(coco_annotations_file, train_augmented_fp)\n",
    "\n",
    "# define a list of colors for drawing bounding boxes\n",
    "color_list = [\"pink\", \"red\", \"teal\", \"blue\", \"orange\", \"yellow\", \"black\", \"magenta\",\"green\",\"aqua\"] * 10\n",
    "num_imgs_to_disp = 4\n",
    "img_ids = coco.get_imgIds()\n",
    "total_images = len(img_ids) # total number of images\n",
    "sel_im_idxs = np.random.permutation(total_images)[:num_imgs_to_disp]\n",
    "selected_img_ids = [img_ids[i] for i in sel_im_idxs]\n",
    "ann_ids = coco.get_annIds(selected_img_ids)\n",
    "im_licenses = coco.get_imgLicenses(selected_img_ids)\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (15, 10))\n",
    "ax = ax.ravel()\n",
    "for i, im in enumerate(selected_img_ids):\n",
    "    image = Image.open(f\"{train_augmented_fp}/{str(im).zfill(0)}.png\")\n",
    "    ann_ids = coco.get_annIds(im)\n",
    "    annotations = coco.load_anns(ann_ids)\n",
    "    for ann in annotations:\n",
    "        _bbox = ann['bbox']\n",
    "        x, y, w, h = [int(b) for b in _bbox]\n",
    "        class_id = ann[\"category_id\"]\n",
    "        class_name = coco.load_cats(class_id)[0][\"name\"]\n",
    "        _license = coco.get_imgLicenses(im)[0][\"name\"]\n",
    "        color_ = color_list[class_id]\n",
    "        rect = plt.Rectangle((x, y), w, h, linewidth = 2, edgecolor = color_, facecolor = 'none')\n",
    "        t_box=ax[i].text(x, y, class_name,  color = 'red', fontsize = 10)\n",
    "        t_box.set_bbox(dict(boxstyle = 'square, pad=0',facecolor = 'white', alpha = 0.6, edgecolor = 'blue'))\n",
    "        ax[i].add_patch(rect)\n",
    "    \n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(image)\n",
    "    ax[i].set_xlabel('Longitude')\n",
    "    ax[i].set_title(f\"License: {_license}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf3f31-6667-417b-bb0a-dd0f055c91fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
