{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# Gradient-Effected Object Recognition Gauge for hive Entrances (GEORGE)\n",
    "Neural-net-powered honeybee hive-mounted pollen, varroa, and wasp counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPs64QA1Zdov"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uZcqD4NLdnf4",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "import pandas as pd\n",
    "import json\n",
    "from GEORGE_Library import *\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a2082fb1e56fc6cfc91d40820b905267bc1ca468",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "category_index = {1: {'id': 1, 'name': 'regular'}, 2: {'id': 2, 'name': 'pollen'}, 3: {'id': 3, 'name': 'varroa'}, 4: {'id': 4, 'name': 'wasps'}}\n",
    "train_augmented_fp = \"Dataset/Custom_Dataset/Train\"\n",
    "val_augmented_fp = \"Dataset/Custom_Dataset/Validation\"\n",
    "model_directory = 'C:/Users/khard/OneDrive/Documents/GitHub/GEORGE/'\n",
    "model_name = 'extract_superimp_model'\n",
    "model_dest = os.path.join(os.sep, model_directory, model_name)\n",
    "tflite_save = True\n",
    "\n",
    "# The `label_id_offset` shifts all classes so the model receives one-hot labels where non-background classes start counting at the zeroth index.\n",
    "label_id_offset = 1\n",
    "score_threshold = 0.3\n",
    "#batch_size = 4\n",
    "epochs = 4\n",
    "#num_batches = round(len(glob.glob(train_augmented_fp + '/*')) / batch_size) # AKA \"Iterations\"\n",
    "num_classes = 4\n",
    "\n",
    "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=learning_rate_boundaries, values=learning_rates\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghDAsqfoZvPh"
   },
   "source": [
    "## Create model and restore weights for all but last layer\n",
    "\n",
    "This cell builds a single stage detection architecture (RetinaNet) and restores all but the classification layer at the top (which will be automatically randomly initialized).  For simplicity, the image size of 640x640 is hardcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9J16r3NChD-7",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doesn't exist\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "retinapath = pathlib.Path('.').absolute() / \"models\" / \"research\" / \"object_detection\" / \"test_data\" / \"checkpoint\"\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if os.path.exists(retinapath):\n",
    "    pass\n",
    "elif not os.path.exists(retinapath):\n",
    "    print(\"Doesn't exist\")\n",
    "    # !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "    # !tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "    # !mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RyT4BUbaMeG-",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and restoring weights for fine-tuning...\n",
      "Restoring from ./tf_ckpts\\ckpt-89933\n",
      "Weights restored!\n"
     ]
    }
   ],
   "source": [
    "# credit: deeplearning.ai (https://github.com/https-deeplearning-ai)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "print('Building model and restoring weights for fine-tuning...', flush=True)\n",
    "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet152_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
    "new_checkpoint_path = './tf_ckpts'\n",
    "\n",
    "# Load pipeline config and build a detection model.\n",
    "#\n",
    "# Since we are working off of a COCO architecture which predicts 90\n",
    "# class slots by default, we override the `num_classes` field\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "model_config.ssd.num_classes = num_classes\n",
    "model_config.ssd.freeze_batchnorm = False\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=True)\n",
    "\n",
    "# Set up object-based checkpoint restore --- RetinaNet has two prediction\n",
    "# `heads` --- one for classification, the other for box regression.  We will\n",
    "# restore the box regression head but initialize the classification head\n",
    "# from scratch (we show the omission below by commenting out the line that\n",
    "# we would add if we wanted to restore both heads)\n",
    "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
    "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "    _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
    "    #    (i.e., the classification head that we *will not* restore)\n",
    "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
    "    )\n",
    "fake_model = tf.compat.v2.train.Checkpoint(\n",
    "          _feature_extractor=detection_model._feature_extractor,\n",
    "          _box_predictor=fake_box_predictor)\n",
    "\n",
    "# Create initial checkpoint with fake model\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model = detection_model, step = tf.Variable(1))\n",
    "ckpt2 = tf.compat.v2.train.Checkpoint(model = fake_model, step = tf.Variable(1))\n",
    "manager = tf.compat.v2.train.CheckpointManager(ckpt, new_checkpoint_path, max_to_keep = 1)\n",
    "\n",
    "# Try to restore checkpoint from tf_ckpts folder.\n",
    "# If none exist, restore from tf models folder\n",
    "batch_num = 0\n",
    "epoch_num = 0\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restoring from {}\".format(manager.latest_checkpoint))\n",
    "    trainParameters = json.load(open(new_checkpoint_path + \"/trainParameters.txt\"))\n",
    "    batch_num = trainParameters[\"batch\"] + 1\n",
    "    epoch_num = trainParameters[\"epoch\"]\n",
    "else:\n",
    "    #ckpt2.restore(checkpoint_path).expect_partial()\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "# Run model through a dummy image so that variables are created\n",
    "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
    "prediction_dict = detection_model.predict(image, shapes)\n",
    "_ = detection_model.postprocess(prediction_dict, shapes)\n",
    "print('Weights restored!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCkWmdoZZ0zJ"
   },
   "source": [
    "## Eager mode custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nyHoF4mUrv5-",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fine-tuning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4:  44%|████████████████████                          | 2727/6250 [10:14:09<13:13:25, 13.51s/batch, loss=0.204]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m image_tensors \u001b[38;5;241m=\u001b[39m [train_image_tensors[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example_keys]\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Training step (forward pass + backwards pass)\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_boxes_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_classes_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m loss_array\u001b[38;5;241m.\u001b[39mappend([total_loss, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     78\u001b[0m ckpt\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39massign_add(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# credit: deeplearning.ai (https://github.com/https-deeplearning-ai)\n",
    "\n",
    "# Select variables in top layers to fine-tune.\n",
    "trainable_variables = detection_model.trainable_variables\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9) # was 0.9\n",
    "train_step_fn = get_model_train_step_function(detection_model, optimizer, trainable_variables)\n",
    "\n",
    "loss_array = []\n",
    "file_list = glob.glob(train_augmented_fp + '/*.png')\n",
    "\n",
    "coco_annotations_file = train_augmented_fp + \"/custom_bee_dataset.json\"\n",
    "coco_images_dir=os.getcwd() + \"\\\\\" + train_augmented_fp\n",
    "coco= COCOParser(coco_annotations_file, coco_images_dir)\n",
    "\n",
    "print('Start fine-tuning!', flush=True)\n",
    "for epoch in range(epoch_num, epochs):\n",
    "    batch_size = 2 * (2**epoch_num)\n",
    "    num_batches = round(len(glob.glob(train_augmented_fp + '/*')) / batch_size) # AKA \"Iterations\"\n",
    "    p = np.random.permutation(len(file_list))\n",
    "    train_random_file_list = [file_list[i] for i in p]\n",
    "    with tqdm(total = num_batches, unit=\"batch\") as pbar:\n",
    "        pbar.set_description(desc=(\"Epoch %i/%i\" % (epoch + 1, epochs)))\n",
    "        for idx in range(batch_num, num_batches):\n",
    "            if (epoch == epoch_num) and (idx == batch_num):\n",
    "                pbar.update(batch_num)\n",
    "            train_images_np = []\n",
    "            gt_classes_one_hot_tensors = []\n",
    "            gt_box_tensors = []\n",
    "            batch_file_list = train_random_file_list[(idx * batch_size):((idx + 1) * batch_size)]\n",
    "            for filename in batch_file_list:\n",
    "                temp_train_labels = []\n",
    "                temp_gt_boxes = []\n",
    "                \n",
    "                img_id = int(Path(filename).stem)\n",
    "                train_images_np.append(np.array(Image.open(filename).convert('RGB')).astype('uint8'))\n",
    "\n",
    "                ann_ids = coco.get_annIds(img_id)\n",
    "                img_w, img_h = coco.get_wh(img_id)\n",
    "                annotations = coco.load_anns(ann_ids)\n",
    "                for i, ann in enumerate(annotations):\n",
    "                    _bbox = ann['bbox']\n",
    "                    x, y, w, h = [int(b) for b in _bbox]\n",
    "                    class_id = ann[\"category_id\"]\n",
    "                    class_name = coco.load_cats(class_id)[0][\"name\"]\n",
    "                    trans_label = class_id - 1\n",
    "\n",
    "                    # Start original script again\n",
    "                    temp_train_labels.append(np.array(tf.one_hot(trans_label, 4)).astype('float32'))\n",
    "                    temp_gt_boxes.append(np.array(coco_to_rel_yxyx(x, y, w, h, img_w, img_h)).astype('float32'))\n",
    "\n",
    "                gt_classes_one_hot_tensors.append(tf.stack(temp_train_labels))\n",
    "                gt_box_tensors.append(tf.stack(temp_gt_boxes))\n",
    "            '''\n",
    "            image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
    "                Note that the height and width can vary across images, as they are\n",
    "                reshaped within this function to be 640x640.\n",
    "              groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
    "                tf.float32 representing groundtruth boxes for each image in the batch.\n",
    "              groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
    "                with type tf.float32 representing groundtruth boxes for each image in\n",
    "                the batch.\n",
    "            '''\n",
    "            \n",
    "            train_image_tensors = prep_train_imgs_only(train_images_np)\n",
    "            # Grab keys for a random subset of examples\n",
    "            all_keys = list(range(len(train_image_tensors) - 1))\n",
    "            random.shuffle(all_keys)\n",
    "            example_keys = all_keys[:batch_size]\n",
    "\n",
    "            gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
    "            gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
    "            image_tensors = [train_image_tensors[key] for key in example_keys]\n",
    "            \n",
    "            # Training step (forward pass + backwards pass)\n",
    "            total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
    "            loss_array.append([total_loss, epoch + 1, idx + 1])\n",
    "            ckpt.step.assign_add(1)\n",
    "\n",
    "            save_path = manager.save()\n",
    "            trainParameters = {\"batch\": idx, \"epoch\": epoch}\n",
    "            json.dump(trainParameters, open(new_checkpoint_path + \"/trainParameters.txt\", \"w\"))\n",
    "            df = pd.DataFrame(loss_array[-1]).T\n",
    "            if (idx == 0) and (epoch == 0):\n",
    "                df = df.rename(columns = {0: \"loss\", 1: \"epoch\", 2: \"idx\"})\n",
    "                df.to_csv('loss_array.csv', mode='w', index=False, header=True)\n",
    "            else:\n",
    "                df.to_csv('loss_array.csv', mode='a', index=False, header=False)\n",
    "            pbar.set_postfix(loss=total_loss.numpy())\n",
    "            pbar.update(1)\n",
    "    batch_num = 0\n",
    "print('Done fine-tuning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(shape=[None,640,640,3], dtype=tf.float32)])\n",
    "\n",
    "def detect(input_tensor): # credit: deeplearning.ai (https://github.com/https-deeplearning-ai)\n",
    "    \"\"\"Run detection on an input image.\n",
    "\n",
    "    Args:\n",
    "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
    "      Note that height and width can be anything since the image will be\n",
    "      immediately resized according to the needs of the model within this\n",
    "      function.\n",
    "\n",
    "    Returns:\n",
    "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
    "      and `detection_scores`).\n",
    "    \"\"\"\n",
    "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
    "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
    "    return detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "tf.saved_model.save(\n",
    "    detection_model , model_dest,\n",
    "    signatures={\n",
    "      'detect': detect.get_concrete_function()\n",
    "    })\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tflite_save:\n",
    "    # Convert the model\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(model_dest) # path to the SavedModel directory\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the model.\n",
    "    with open('model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_array = pd.read_csv('loss_array.csv')\n",
    "y_loss = []\n",
    "E_loss = []\n",
    "x_loss = []\n",
    "\n",
    "for i in final_loss_array.iterrows():\n",
    "    y_loss.append(float(i[1][0].split(',')[0].replace(\"tf.Tensor(\",'')))\n",
    "    E_loss.append(int(i[1][1]))\n",
    "    x_loss.append(int(i[1][2]))\n",
    "\n",
    "loss_csv = pd.DataFrame()\n",
    "loss_csv['y_loss'] = y_loss\n",
    "loss_csv['E_loss'] = E_loss\n",
    "loss_csv['x_loss'] = x_loss\n",
    "\n",
    "filepath = Path('loss_plot.csv')\n",
    "loss_csv.to_csv(filepath)\n",
    "    \n",
    "for i in range(len(E_loss)):\n",
    "    if E_loss[i] > 1:\n",
    "        x_loss[i] = x_loss[i] + list(E_loss).count(1) * (E_loss[i] - 1)\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax1 = host_subplot(111, axes_class=AA.Axes)\n",
    "plt.title(\"GEORGE Training Loss vs. Batches\")\n",
    "ax2 = ax1.twiny()\n",
    "new_fixed_axis = ax2.get_grid_helper().new_fixed_axis\n",
    "\n",
    "# First X-axis\n",
    "ax1.plot(x_loss, y_loss, 'b')\n",
    "kalman_y_loss = kalman(y_loss, 0.0001)\n",
    "ax1.plot(x_loss, kalman_y_loss, 'orange')\n",
    "ax1.set_xticks([int(i) for i in np.linspace(0, len(x_loss), 10)])\n",
    "ax1.yaxis.set_label_text(\"Loss\")\n",
    "ax1.xaxis.set_label_text(\"Batch\")\n",
    "for i,j in zip([int(i) for i in np.linspace(0, len(x_loss), 10)], [round(kalman_y_loss[int(i - 1)], 3) if i > 1 else round(kalman_y_loss[int(i)], 3) for i in np.linspace(0, len(x_loss), 10)]):\n",
    "    ax1.annotate(str(j), xy=(i - 220, j + 0.2))\n",
    "\n",
    "# Second X-axis\n",
    "offset = 0, -35 # Position of the second axis\n",
    "ax2.axis[\"bottom\"] = new_fixed_axis(loc=\"bottom\", axes=ax2, offset=offset)\n",
    "ax2.axis[\"top\"].set_visible(False)\n",
    "\n",
    "# Need to change this to scale axis divisions dynamically with different number of epochs\n",
    "ax2.set_xticks([i * num_batches for i in range(epochs + 1)])\n",
    "ax2.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "ax2.xaxis.set_minor_locator(ticker.FixedLocator([int((num_batches / 2) + (num_batches * i)) for i in range(epochs)]))\n",
    "ax2.xaxis.set_minor_formatter(ticker.FixedFormatter([\"Epoch %s\" % str(i + 1) for i in range(epochs)]))\n",
    "\n",
    "ax1.grid(1)\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "plt.savefig('GEORGE_Loss_vs_Batch.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "interactive_eager_few_shot_od_training_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
