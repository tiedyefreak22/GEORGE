{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# Gradient-Effected Object Recognition Gauge for hive Entrances (GEORGE)\n",
    "Neural-net-powered honeybee hive-mounted pollen, varroa, and wasp counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPs64QA1Zdov"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uZcqD4NLdnf4",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import host_subplot \n",
    "import mpl_toolkits.axisartist as AA\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "import pandas as pd\n",
    "import json\n",
    "from GEORGE_Library import *\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a2082fb1e56fc6cfc91d40820b905267bc1ca468",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "category_index = {1: {'id': 1, 'name': 'regular'}, 2: {'id': 2, 'name': 'pollen'}, 3: {'id': 3, 'name': 'varroa'}, 4: {'id': 4, 'name': 'wasps'}}\n",
    "train_augmented_fp = \"Dataset/Custom_Dataset/Train\"\n",
    "val_augmented_fp = \"Dataset/Custom_Dataset/Validation\"\n",
    "model_directory = 'C:/Users/khard/OneDrive/Documents/GitHub/GEORGE/'\n",
    "model_name = 'extract_superimp_model'\n",
    "model_dest = os.path.join(os.sep, model_directory, model_name)\n",
    "tflite_save = True\n",
    "\n",
    "# The `label_id_offset` shifts all classes so the model receives one-hot labels where non-background classes start counting at the zeroth index.\n",
    "label_id_offset = 1\n",
    "score_threshold = 0.3\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "num_batches = round(len(glob.glob(train_augmented_fp + '/*')) / batch_size) # AKA \"Iterations\"\n",
    "num_classes = 4\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghDAsqfoZvPh"
   },
   "source": [
    "## Create model and restore weights for all but last layer\n",
    "\n",
    "This cell builds a single stage detection architecture (RetinaNet) and restores all but the classification layer at the top (which will be automatically randomly initialized).  For simplicity, the image size of 640x640 is hardcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9J16r3NChD-7",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "retinapath = pathlib.Path('.').absolute() / \"models\" / \"research\" / \"object_detection\" / \"test_data\" / \"checkpoint\"\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if os.path.exists(retinapath):\n",
    "    pass\n",
    "elif not os.path.exists(retinapath):\n",
    "    print(\"Doesn't exist\")\n",
    "    # !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "    # !tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "    # !mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RyT4BUbaMeG-",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and restoring weights for fine-tuning...\n",
      "Restoring from ./tf_ckpts\\ckpt-37\n",
      "Weights restored!\n"
     ]
    }
   ],
   "source": [
    "# credit: deeplearning.ai (https://github.com/https-deeplearning-ai)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "print('Building model and restoring weights for fine-tuning...', flush=True)\n",
    "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
    "new_checkpoint_path = './tf_ckpts'\n",
    "\n",
    "# Load pipeline config and build a detection model.\n",
    "#\n",
    "# Since we are working off of a COCO architecture which predicts 90\n",
    "# class slots by default, we override the `num_classes` field\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "model_config.ssd.num_classes = num_classes\n",
    "model_config.ssd.freeze_batchnorm = True\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=True)\n",
    "\n",
    "# Set up object-based checkpoint restore --- RetinaNet has two prediction\n",
    "# `heads` --- one for classification, the other for box regression.  We will\n",
    "# restore the box regression head but initialize the classification head\n",
    "# from scratch (we show the omission below by commenting out the line that\n",
    "# we would add if we wanted to restore both heads)\n",
    "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
    "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
    "    #    (i.e., the classification head that we *will not* restore)\n",
    "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
    "    )\n",
    "fake_model = tf.compat.v2.train.Checkpoint(\n",
    "          _feature_extractor=detection_model._feature_extractor,\n",
    "          _box_predictor=fake_box_predictor)\n",
    "\n",
    "# Create initial checkpoint with fake model\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model = fake_model, step = tf.Variable(1))\n",
    "manager = tf.compat.v2.train.CheckpointManager(ckpt, new_checkpoint_path, max_to_keep = 1)\n",
    "\n",
    "# Try to restore checkpoint from tf_ckpts folder.\n",
    "# If none exist, restore from tf models folder\n",
    "batch_num = 0\n",
    "epoch_num = 0\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restoring from {}\".format(manager.latest_checkpoint))\n",
    "    trainParameters = json.load(open(new_checkpoint_path + \"/trainParameters.txt\"))\n",
    "    batch_num = trainParameters[\"batch\"]\n",
    "    epoch_num = trainParameters[\"epoch\"]\n",
    "else:\n",
    "    ckpt.restore(checkpoint_path).expect_partial()\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "# Run model through a dummy image so that variables are created\n",
    "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
    "prediction_dict = detection_model.predict(image, shapes)\n",
    "_ = detection_model.postprocess(prediction_dict, shapes)\n",
    "print('Weights restored!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCkWmdoZZ0zJ"
   },
   "source": [
    "## Eager mode custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyHoF4mUrv5-",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fine-tuning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  66%|██████████████████████████████▉                | 1027/1563 [19:47:17<9:58:25, 66.99s/batch, loss=0.457]"
     ]
    }
   ],
   "source": [
    "# credit: deeplearning.ai (https://github.com/https-deeplearning-ai)\n",
    "\n",
    "# Select variables in top layers to fine-tune.\n",
    "trainable_variables = detection_model.trainable_variables\n",
    "to_fine_tune = []\n",
    "prefixes_to_train = [\n",
    "    'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
    "    'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
    "for var in trainable_variables:\n",
    "    if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
    "        to_fine_tune.append(var)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "train_step_fn = get_model_train_step_function(detection_model, optimizer, to_fine_tune)\n",
    "\n",
    "loss_array = []\n",
    "file_list = glob.glob(train_augmented_fp + '/*.png')\n",
    "\n",
    "coco_annotations_file = train_augmented_fp + \"/custom_bee_dataset.json\"\n",
    "coco_images_dir=os.getcwd() + \"\\\\\" + train_augmented_fp\n",
    "coco= COCOParser(coco_annotations_file, coco_images_dir)\n",
    "\n",
    "start_flag = 1\n",
    "print('Start fine-tuning!', flush=True)\n",
    "for epoch in range(epochs - epoch_num):\n",
    "    p = np.random.permutation(len(file_list))\n",
    "    train_random_file_list = [file_list[i] for i in p]\n",
    "    with tqdm(total = num_batches, unit=\"batch\") as pbar:\n",
    "        pbar.set_description(desc=(\"Epoch %i\" % (epoch_num + 1) + \"/%i\" % epochs))\n",
    "        if start_flag:\n",
    "            pbar.update(batch_num)\n",
    "            start_flag = 0\n",
    "        for idx in range(num_batches - batch_num):\n",
    "            train_images_np = []\n",
    "            gt_classes_one_hot_tensors = []\n",
    "            gt_box_tensors = []\n",
    "            batch_file_list = train_random_file_list[(idx * batch_size):((idx + 1) * batch_size)]\n",
    "            for filename in batch_file_list:\n",
    "                temp_train_labels = []\n",
    "                temp_gt_boxes = []\n",
    "                \n",
    "                img_id = int(Path(filename).stem)\n",
    "                train_images_np.append(np.array(Image.open(filename).convert('RGB')).astype('uint8'))\n",
    "\n",
    "                ann_ids = coco.get_annIds(img_id)\n",
    "                img_w, img_h = coco.get_wh(img_id)\n",
    "                annotations = coco.load_anns(ann_ids)\n",
    "                for i, ann in enumerate(annotations):\n",
    "                    _bbox = ann['bbox']\n",
    "                    x, y, w, h = [int(b) for b in _bbox]\n",
    "                    class_id = ann[\"category_id\"]\n",
    "                    class_name = coco.load_cats(class_id)[0][\"name\"]\n",
    "                    trans_label = class_id - 1\n",
    "\n",
    "                    # Start original script again\n",
    "                    temp_train_labels.append(np.array(tf.one_hot(trans_label, 4)).astype('float32'))\n",
    "                    temp_gt_boxes.append(np.array(coco_to_rel_yxyx(x, y, w, h, img_w, img_h)).astype('float32'))\n",
    "\n",
    "                gt_classes_one_hot_tensors.append(tf.stack(temp_train_labels))\n",
    "                gt_box_tensors.append(tf.stack(temp_gt_boxes))\n",
    "            '''\n",
    "            image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
    "                Note that the height and width can vary across images, as they are\n",
    "                reshaped within this function to be 640x640.\n",
    "              groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
    "                tf.float32 representing groundtruth boxes for each image in the batch.\n",
    "              groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
    "                with type tf.float32 representing groundtruth boxes for each image in\n",
    "                the batch.\n",
    "            '''\n",
    "            \n",
    "            train_image_tensors = prep_train_imgs_only(train_images_np)\n",
    "            # Grab keys for a random subset of examples\n",
    "            all_keys = list(range(len(train_image_tensors) - 1))\n",
    "            random.shuffle(all_keys)\n",
    "            example_keys = all_keys[:batch_size]\n",
    "\n",
    "            gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
    "            gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
    "            image_tensors = [train_image_tensors[key] for key in example_keys]\n",
    "            \n",
    "            # Training step (forward pass + backwards pass)\n",
    "            total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
    "            loss_array.append([total_loss, epoch + 1, idx])\n",
    "            ckpt.step.assign_add(1)\n",
    "\n",
    "            save_path = manager.save()\n",
    "            trainParameters = {\"batch\": idx + 1, \"epoch\": epoch}\n",
    "            json.dump(trainParameters, open(new_checkpoint_path + \"/trainParameters.txt\", \"w\"))\n",
    "            pbar.set_postfix(loss=total_loss.numpy())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(\"Saved checkpoint for epoch {}: {}\".format(int(ckpt.step), save_path))\n",
    "print('Done fine-tuning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(shape=[None,640,640,3], dtype=tf.float32)])\n",
    "\n",
    "def detect(input_tensor): # credit: deeplearning.ai (https://github.com/https-deeplearning-ai)\n",
    "    \"\"\"Run detection on an input image.\n",
    "\n",
    "    Args:\n",
    "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
    "      Note that height and width can be anything since the image will be\n",
    "      immediately resized according to the needs of the model within this\n",
    "      function.\n",
    "\n",
    "    Returns:\n",
    "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
    "      and `detection_scores`).\n",
    "    \"\"\"\n",
    "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
    "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
    "    return detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "tf.saved_model.save(\n",
    "    detection_model , model_dest,\n",
    "    signatures={\n",
    "      'detect': detect.get_concrete_function()\n",
    "    })\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tflite_save:\n",
    "    # Convert the model\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(model_dest) # path to the SavedModel directory\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the model.\n",
    "    with open('model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'loss_array' in locals() or 'loss_array' in globals():\n",
    "    y_loss = []\n",
    "    E_loss = []\n",
    "    x_loss = []\n",
    "\n",
    "    for i in loss_array:\n",
    "        y_loss.append(float(i[0]))\n",
    "        E_loss.append(i[1])\n",
    "        x_loss.append(i[2])\n",
    "\n",
    "    loss_csv = pd.DataFrame()\n",
    "    loss_csv['y_loss'] = y_loss\n",
    "    loss_csv['E_loss'] = E_loss\n",
    "    loss_csv['x_loss'] = x_loss\n",
    "\n",
    "    print(loss_csv)\n",
    "    filepath = Path('loss_plot.csv')\n",
    "    loss_csv.to_csv(filepath)\n",
    "else:\n",
    "    loss_array = pd.read_csv('loss_plot.csv')\n",
    "    y_loss = loss_array[\"y_loss\"]\n",
    "    E_loss = loss_array[\"E_loss\"]\n",
    "    x_loss = loss_array[\"x_loss\"]\n",
    "    \n",
    "for i in range(len(E_loss)):\n",
    "    if E_loss[i] == 2:\n",
    "        x_loss[i] = x_loss[i] + list(E_loss).count(1)\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax1 = host_subplot(111, axes_class=AA.Axes)\n",
    "plt.title(\"GEORGE Training Loss vs. Batches\")\n",
    "ax2 = ax1.twiny()\n",
    "new_fixed_axis = ax2.get_grid_helper().new_fixed_axis\n",
    "\n",
    "# First X-axis\n",
    "ax1.plot(x_loss, y_loss)\n",
    "ax1.set_xticks([int(i) for i in np.linspace(0, len(x_loss), 10)])\n",
    "ax1.yaxis.set_label_text(\"Loss\")\n",
    "ax1.xaxis.set_label_text(\"Batch\")\n",
    "\n",
    "# Second X-axis\n",
    "offset = 0, -35 # Position of the second axis\n",
    "ax2.axis[\"bottom\"] = new_fixed_axis(loc=\"bottom\", axes=ax2, offset=offset)\n",
    "ax2.axis[\"top\"].set_visible(False)\n",
    "\n",
    "# Need to change this to scale axis divisions dynamically with different number of epochs\n",
    "ax2.set_xticks([0, list(E_loss).count(1), len(E_loss)])\n",
    "ax2.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "ax2.xaxis.set_minor_locator(ticker.FixedLocator([len(E_loss)/4, (len(E_loss)/4)*3]))\n",
    "ax2.xaxis.set_minor_formatter(ticker.FixedFormatter([\"Epoch 1\", \"Epoch 2\"]))\n",
    "\n",
    "ax1.grid(1)\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "plt.savefig('GEORGE_Loss_vs_Batch.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "interactive_eager_few_shot_od_training_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
