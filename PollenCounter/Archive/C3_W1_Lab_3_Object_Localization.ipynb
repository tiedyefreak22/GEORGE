{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsjDCIat6_UK"
   },
   "source": [
    "# Image Classification and Object Localization\n",
    "\n",
    "This notebook:\n",
    "- classifies the main subject in an image\n",
    "- localizes it by drawing bounding boxes around it.\n",
    "- Places each image on a canvas of width 640x640 at random locations.\n",
    "- Calculates the corresponding bounding boxes for those images.\n",
    "\n",
    "The bounding box prediction can be modelled as a \"regression\" task, which means that the model will predict a numeric value (as opposed to a category)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpiJj8ym0v0-"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AoilhmYe1b5t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os, re, time, json\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmoFKEd98MP3"
   },
   "source": [
    "# Visualization Utilities\n",
    "\n",
    "These functions are used to draw bounding boxes around the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tBjj1Fg-i_lc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#@title Plot Utilities for Bounding Boxes [RUN ME]\n",
    "\n",
    "im_width = 640\n",
    "im_height = 640\n",
    "use_normalized_coordinates = True\n",
    "\n",
    "def draw_bounding_boxes_on_image_array(image,\n",
    "                                       boxes,\n",
    "                                       color=[],\n",
    "                                       thickness=1,\n",
    "                                       display_str_list=()):\n",
    "  \"\"\"Draws bounding boxes on image (numpy array).\n",
    "  Args:\n",
    "    image: a numpy array object.\n",
    "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "           The coordinates are in normalized format between [0, 1].\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list_list: a list of strings for each bounding box.\n",
    "  Raises:\n",
    "    ValueError: if boxes is not a [N, 4] array\n",
    "  \"\"\"\n",
    "  image_pil = PIL.Image.fromarray(image)\n",
    "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
    "  rgbimg.paste(image_pil)\n",
    "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness,\n",
    "                               display_str_list)\n",
    "  return np.array(rgbimg)\n",
    "  \n",
    "\n",
    "def draw_bounding_boxes_on_image(image,\n",
    "                                 boxes,\n",
    "                                 color=[],\n",
    "                                 thickness=1,\n",
    "                                 display_str_list=()):\n",
    "  \"\"\"Draws bounding boxes on image.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "           The coordinates are in normalized format between [0, 1].\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: a list of strings for each bounding box.\n",
    "                           \n",
    "  Raises:\n",
    "    ValueError: if boxes is not a [N, 4] array\n",
    "  \"\"\"\n",
    "  boxes_shape = boxes.shape\n",
    "  if not boxes_shape:\n",
    "    return\n",
    "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
    "    raise ValueError('Input must be of size [N, 4]')\n",
    "  for i in range(boxes_shape[0]):\n",
    "    draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3],\n",
    "                               boxes[i, 2], color[i], thickness, display_str_list[i])\n",
    "        \n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color='red',\n",
    "                               thickness=1,\n",
    "                               display_str=None,\n",
    "                               use_normalized_coordinates=True):\n",
    "  \"\"\"Adds a bounding box to an image.\n",
    "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    ymin: ymin of bounding box.\n",
    "    xmin: xmin of bounding box.\n",
    "    ymax: ymax of bounding box.\n",
    "    xmax: xmax of bounding box.\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: string to display in box\n",
    "    use_normalized_coordinates: If True (default), treat coordinates\n",
    "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
    "      coordinates as absolute.\n",
    "  \"\"\"\n",
    "  draw = PIL.ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "             (right, top), (left, top)], width=thickness, fill=color)\n",
    "  \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USx9tRBF8hWy"
   },
   "source": [
    "These utilities are used to visualize the data and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qhdz68Xm3Z4Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#@title Visualization Utilities [RUN ME]\n",
    "\"\"\"\n",
    "This cell contains helper functions used for visualization\n",
    "and downloads only. \n",
    "\n",
    "You can skip reading it, as there is very\n",
    "little Keras or Tensorflow related code here.\n",
    "\"\"\"\n",
    "\n",
    "# Matplotlib config\n",
    "plt.rc('image', cmap='gray')\n",
    "plt.rc('grid', linewidth=0)\n",
    "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
    "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
    "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
    "plt.rc('text', color='a8151a')\n",
    "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
    "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
    "\n",
    "# pull a batch from the datasets. This code is not very nice, it gets much better in eager mode (TODO)\n",
    "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
    "  \n",
    "  # get one batch from each: 10000 validation digits, N training digits\n",
    "  batch_train_ds = training_dataset.unbatch().batch(N)\n",
    "  \n",
    "  # eager execution: loop through datasets normally\n",
    "  if tf.executing_eagerly():\n",
    "    for validation_digits, (validation_labels, validation_bboxes) in validation_dataset:\n",
    "      validation_digits = validation_digits.numpy()\n",
    "      validation_labels = validation_labels.numpy()\n",
    "      validation_bboxes = validation_bboxes.numpy()\n",
    "      break\n",
    "    for training_digits, (training_labels, training_bboxes) in batch_train_ds:\n",
    "      training_digits = training_digits.numpy()\n",
    "      training_labels = training_labels.numpy()\n",
    "      training_bboxes = training_bboxes.numpy()\n",
    "      break\n",
    "  \n",
    "  # these were one-hot encoded in the dataset\n",
    "  validation_labels = np.argmax(validation_labels, axis=1)\n",
    "  training_labels = np.argmax(training_labels, axis=1)\n",
    "  \n",
    "  return (training_digits, training_labels, training_bboxes,\n",
    "          validation_digits, validation_labels, validation_bboxes)\n",
    "\n",
    "# create digits from local fonts for testing\n",
    "'''\n",
    "def create_digits_from_local_fonts(n):\n",
    "  font_labels = []\n",
    "  img = PIL.Image.new('LA', (75*n, 75), color = (0,255)) # format 'LA': black in channel 0, alpha in channel 1\n",
    "  font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\n",
    "  font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\n",
    "  d = PIL.ImageDraw.Draw(img)\n",
    "  for i in range(n):\n",
    "    font_labels.append(i%10)\n",
    "    d.text((7+i*75,0 if i<10 else -4), str(i%10), fill=(255,255), font=font1 if i<10 else font2)\n",
    "  font_digits = np.array(img.getdata(), np.float32)[:,0] / 255.0 # black in channel 0, alpha in channel 1 (discarded)\n",
    "  font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [75, 75*n]), n, axis=1), axis=0), [n, 75*75])\n",
    "  return font_digits, font_labels\n",
    "'''\n",
    "\n",
    "# utility to display a row of digits with their predictions\n",
    "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, iou, title):\n",
    "\n",
    "  n = 10\n",
    "\n",
    "  indexes = np.random.choice(len(predictions), size=n)\n",
    "  n_digits = digits[indexes]\n",
    "  n_predictions = predictions[indexes]\n",
    "  n_labels = labels[indexes]\n",
    "\n",
    "  n_iou = []\n",
    "  if len(iou) > 0:\n",
    "    n_iou = iou[indexes]\n",
    "\n",
    "  if (len(pred_bboxes) > 0):\n",
    "    n_pred_bboxes = pred_bboxes[indexes,:]\n",
    "\n",
    "  if (len(bboxes) > 0):\n",
    "    n_bboxes = bboxes[indexes,:]\n",
    "\n",
    "\n",
    "  n_digits = n_digits * 255.0\n",
    "  n_digits = n_digits.reshape(n, 75, 75)\n",
    "  fig = plt.figure(figsize=(20, 4))\n",
    "  plt.title(title)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  \n",
    "  for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i+1)\n",
    "    bboxes_to_plot = []\n",
    "    if (len(pred_bboxes) > i):\n",
    "      bboxes_to_plot.append(n_pred_bboxes[i])\n",
    "    \n",
    "    if (len(bboxes) > i):\n",
    "      bboxes_to_plot.append(n_bboxes[i])\n",
    "\n",
    "    img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes=np.asarray(bboxes_to_plot), color=['red', 'green'], display_str_list=[\"true\", \"pred\"])\n",
    "    plt.xlabel(n_predictions[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    if n_predictions[i] != n_labels[i]:\n",
    "      ax.xaxis.label.set_color('red')\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.imshow(img_to_draw)\n",
    "\n",
    "    if len(iou) > i :\n",
    "      color = \"black\"\n",
    "      if (n_iou[i][0] < iou_threshold):\n",
    "        color = \"red\"\n",
    "      ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i][0]), color=color, transform=ax.transAxes)\n",
    "\n",
    "\n",
    "# utility to display training and validation curves\n",
    "def plot_metrics(metric_name, title, ylim=5):\n",
    "  plt.title(title)\n",
    "  plt.ylim(0,ylim)\n",
    "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
    "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ok__0RB-M8S"
   },
   "source": [
    "## Selecting Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Hd5zB1G7Y9-7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n",
      "Number of accelerators:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware\n",
    "strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "print('Running on CPU')\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lvo0t7XVIkWZ"
   },
   "source": [
    "### Parameters\n",
    "\n",
    "The global batch size is the batch size per replica (64 in this case) times the number of replicas in the distribution strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cCpkS9C_H7Tl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync # Global batch size.\n",
    "# The global batch size will be automatically shared across all\n",
    "# replicas by the tf.data.Dataset API. The best practice is to scale the batch size by the number of\n",
    "# replicas (cores). The learning rate should be increased as well.\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVkc7nzg-WUy"
   },
   "source": [
    "## Loading and Preprocessing the Dataset\n",
    "\n",
    "Define some helper functions that will pre-process your data:\n",
    "- `read_image_tfds`: randomly overlays the \"digit\" image on top of a larger canvas.\n",
    "- `get_training_dataset`: loads data and splits it to get the training set.\n",
    "- `get_validation_dataset`: loads and splits the data to get the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZE8dgyPC1_6m",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 08:11:13.114739: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "<_RepeatDataset element_spec=(TensorSpec(shape=(2000, 150, 75, 3), dtype=tf.uint8, name=None), {'cooling_output': TensorSpec(shape=(2000,), dtype=tf.float64, name=None), 'pollen_output': TensorSpec(shape=(2000,), dtype=tf.float64, name=None), 'varroa_output': TensorSpec(shape=(2000,), dtype=tf.float64, name=None), 'wasps_output': TensorSpec(shape=(2000,), dtype=tf.float64, name=None)})>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Transforms each image in dataset by pasting it on a 640x640 canvas at random locations.\n",
    "'''\n",
    "\n",
    "def read_image_tfds(image, label):\n",
    "    xmin = tf.random.uniform((), 0 , 488, dtype=tf.int32)\n",
    "    ymin = tf.random.uniform((), 0 , 563, dtype=tf.int32)\n",
    "    image = tf.reshape(tf.image.rgb_to_grayscale(image), (150,75,1,))\n",
    "    image = tf.image.pad_to_bounding_box(image, ymin, xmin, 640, 640)\n",
    "    image = tf.cast(image, tf.float32)/255.0\n",
    "    xmin = tf.cast(xmin, tf.float32)\n",
    "    ymin = tf.cast(ymin, tf.float32)\n",
    "   \n",
    "    xmax = (xmin + 150) / 640\n",
    "    ymax = (ymin + 75) / 640\n",
    "    xmin = xmin / 640\n",
    "    ymin = ymin / 640\n",
    "    return image, (tf.one_hot(label, 10), [xmin, ymin, xmax, ymax])\n",
    "  \n",
    "'''\n",
    "Loads and maps the training split of the dataset using the map function. Note that we try to load the gcs version since TPU can only work with datasets on Google Cloud Storage.\n",
    "'''\n",
    "def get_training_dataset():\n",
    "      \n",
    "      with  strategy.scope():\n",
    "        dataset = tfds.load(\"bee_dataset/bee_dataset_150\", split=\"train\", as_supervised=True, try_gcs=True)\n",
    "        #dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
    "        dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n",
    "        dataset = dataset.repeat() # Mandatory for Keras for now\n",
    "        dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
    "        dataset = dataset.prefetch(-1)  # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
    "      return dataset\n",
    "\n",
    "'''\n",
    "Loads and maps the validation split of the dataset using the map function. Note that we try to load the gcs version since TPU can only work with datasets on Google Cloud Storage.\n",
    "'''  \n",
    "def get_validation_dataset():\n",
    "    dataset = tfds.load(\"bee_dataset/bee_dataset_150\", split=\"train\", as_supervised=True, try_gcs=True)\n",
    "    #dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
    "\n",
    "    #dataset = dataset.cache() # this small dataset can be entirely cached in RAM\n",
    "    dataset = dataset.batch(2000, drop_remainder=True) # 10000 items in eval dataset, all in one batch\n",
    "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
    "    return dataset\n",
    "\n",
    "# instantiate the datasets\n",
    "with strategy.scope():\n",
    "  training_dataset = get_training_dataset()\n",
    "  validation_dataset = get_validation_dataset()\n",
    "\n",
    "print(\"Done\")\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fXo6GuvL3EB"
   },
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yZ4tjPKvL2eh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 18:25:47.726874: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m (training_digits, training_labels, training_bboxes,\n\u001b[0;32m----> 2\u001b[0m  validation_digits, validation_labels, validation_bboxes) \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_to_numpy_util\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m display_digits_with_boxes(training_digits, training_labels, training_labels, np\u001b[38;5;241m.\u001b[39marray([]), training_bboxes, np\u001b[38;5;241m.\u001b[39marray([]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining digits and their labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m display_digits_with_boxes(validation_digits, validation_labels, validation_labels, np\u001b[38;5;241m.\u001b[39marray([]), validation_bboxes, np\u001b[38;5;241m.\u001b[39marray([]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation digits and their labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mdataset_to_numpy_util\u001b[0;34m(training_dataset, validation_dataset, N)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# eager execution: loop through datasets normally\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m---> 28\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m validation_digits, (validation_labels, validation_bboxes) \u001b[38;5;129;01min\u001b[39;00m validation_dataset:\n\u001b[1;32m     29\u001b[0m     validation_digits \u001b[38;5;241m=\u001b[39m validation_digits\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     30\u001b[0m     validation_labels \u001b[38;5;241m=\u001b[39m validation_labels\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "(training_digits, training_labels, training_bboxes,\n",
    " validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(training_dataset, validation_dataset, 10)\n",
    "\n",
    "display_digits_with_boxes(training_digits, training_labels, training_labels, np.array([]), training_bboxes, np.array([]), \"training digits and their labels\")\n",
    "display_digits_with_boxes(validation_digits, validation_labels, validation_labels, np.array([]), validation_bboxes, np.array([]), \"validation digits and their labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8nHWWkS_eeZ"
   },
   "source": [
    "## Define the Network\n",
    "\n",
    "Here, you'll define your custom CNN. \n",
    "- `feature_extractor`: these convolutional layers extract the features of the image.\n",
    "- `classifier`:  This define the output layer that predicts among 10 categories (digits 0 through 9)\n",
    "- `bounding_box_regression`: This defines the output layer that predicts 4 numeric values, which define the coordinates of the bounding box (xmin, ymin, xmax, ymax)\n",
    "- `final_model`: This combines the layers for feature extraction, classification and bounding box prediction.  \n",
    "  - Notice that this is another example of a branching model, because the model splits to produce two kinds of output (a category and set of numbers).  \n",
    "  - Since you've learned to use the Functional API earlier in the specialization (course 1), you have the flexibility to define this kind of branching model!\n",
    "- `define_and_compile_model`: choose the optimizer and metrics, then compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "56y8UNFQIVwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 150, 75, 3)]         0         []                            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 148, 73, 16)          448       ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (Avera  (None, 74, 36, 16)           0         ['conv2d_3[0][0]']            \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 72, 34, 32)           4640      ['average_pooling2d_3[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_4 (Avera  (None, 36, 17, 32)           0         ['conv2d_4[0][0]']            \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 34, 15, 64)           18496     ['average_pooling2d_4[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_5 (Avera  (None, 17, 7, 64)            0         ['conv2d_5[0][0]']            \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 7616)                 0         ['average_pooling2d_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 128)                  974976    ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " classification (Dense)      (None, 10)                   1290      ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " bounding_box (Dense)        (None, 4)                    516       ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1000366 (3.82 MB)\n",
      "Trainable params: 1000366 (3.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Feature extractor is the CNN that is made up of convolution and pooling layers.\n",
    "'''\n",
    "def feature_extractor(inputs):\n",
    "    x = tf.keras.layers.Conv2D(16, activation='relu', kernel_size=3, input_shape=(640, 640, 1))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "'''\n",
    "dense_layers adds a flatten and dense layer.\n",
    "This will follow the feature extraction layers\n",
    "'''\n",
    "def dense_layers(inputs):\n",
    "  x = tf.keras.layers.Flatten()(inputs)\n",
    "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "'''\n",
    "Classifier defines the classification output.\n",
    "This has a set of fully connected layers and a softmax layer.\n",
    "'''\n",
    "def classifier(inputs):\n",
    "\n",
    "  classification_output = tf.keras.layers.Dense(10, activation='softmax', name = 'classification')(inputs)\n",
    "  return classification_output\n",
    "\n",
    "\n",
    "'''\n",
    "This function defines the regression output for bounding box prediction. \n",
    "Note that we have four outputs corresponding to (xmin, ymin, xmax, ymax)\n",
    "'''\n",
    "def bounding_box_regression(inputs):\n",
    "    bounding_box_regression_output = tf.keras.layers.Dense(units = '4', name = 'bounding_box')(inputs)\n",
    "    return bounding_box_regression_output\n",
    "\n",
    "\n",
    "def final_model(inputs):\n",
    "    feature_cnn = feature_extractor(inputs)\n",
    "    dense_output = dense_layers(feature_cnn)\n",
    "\n",
    "    '''\n",
    "    The model branches here.  \n",
    "    The dense layer's output gets fed into two branches:\n",
    "    classification_output and bounding_box_output\n",
    "    '''\n",
    "    classification_output = classifier(dense_output)\n",
    "    bounding_box_output = bounding_box_regression(dense_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = [classification_output, bounding_box_output])\n",
    "\n",
    "    return model\n",
    "  \n",
    "\n",
    "def define_and_compile_model(inputs):\n",
    "  model = final_model(inputs)\n",
    "  \n",
    "  model.compile(optimizer='adam', \n",
    "              loss = {'classification' : 'categorical_crossentropy',\n",
    "                      'bounding_box' : 'mse'\n",
    "                     },\n",
    "              metrics = {'classification' : 'accuracy',\n",
    "                         'bounding_box' : 'mse'\n",
    "                        })\n",
    "  return model\n",
    "\n",
    "    \n",
    "with strategy.scope():\n",
    "  inputs = tf.keras.layers.Input(shape=(150, 75, 3,))\n",
    "  model = define_and_compile_model(inputs)\n",
    "\n",
    "# print model layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuhDh8ao8VyB"
   },
   "source": [
    "### Train and validate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv0BQTPsKrkt"
   },
   "source": [
    "Train the model.  \n",
    "- You can choose the number of epochs depending on the level of performance that you want and the time that you have.\n",
    "- Each epoch will take just a few seconds if you're using the TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TTwH_P-ZJ_xx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 236, in __call__\n        y_true = self._conform_to_outputs(y_pred, y_true)\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 60, in _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 805, in map_to_output_names\n        raise ValueError(\n\n    ValueError: Found unexpected losses or metrics that do not correspond to any Model output: dict_keys(['cooling_output', 'pollen_output', 'varroa_output', 'wasps_output']). Valid mode output names: ['classification', 'bounding_box']. Received struct is: {'cooling_output': <tf.Tensor 'IteratorGetNext:1' shape=(64,) dtype=float64>, 'pollen_output': <tf.Tensor 'IteratorGetNext:2' shape=(64,) dtype=float64>, 'varroa_output': <tf.Tensor 'IteratorGetNext:3' shape=(64,) dtype=float64>, 'wasps_output': <tf.Tensor 'IteratorGetNext:4' shape=(64,) dtype=float64>}.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7507\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mBATCH_SIZE  \u001b[38;5;66;03m# 60,000 items in this dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m validation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m loss, classification_loss, bounding_box_loss, classification_accuracy, bounding_box_mse \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(validation_dataset, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, classification_accuracy)\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileqmanm3iv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 236, in __call__\n        y_true = self._conform_to_outputs(y_pred, y_true)\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 60, in _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    File \"/home/kevinhardin/mambaforge/envs/tensorflow/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 805, in map_to_output_names\n        raise ValueError(\n\n    ValueError: Found unexpected losses or metrics that do not correspond to any Model output: dict_keys(['cooling_output', 'pollen_output', 'varroa_output', 'wasps_output']). Valid mode output names: ['classification', 'bounding_box']. Received struct is: {'cooling_output': <tf.Tensor 'IteratorGetNext:1' shape=(64,) dtype=float64>, 'pollen_output': <tf.Tensor 'IteratorGetNext:2' shape=(64,) dtype=float64>, 'varroa_output': <tf.Tensor 'IteratorGetNext:3' shape=(64,) dtype=float64>, 'wasps_output': <tf.Tensor 'IteratorGetNext:4' shape=(64,) dtype=float64>}.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10 # 45\n",
    "steps_per_epoch = 7507//BATCH_SIZE  # 60,000 items in this dataset\n",
    "validation_steps = 1\n",
    "\n",
    "history = model.fit(training_dataset,\n",
    "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)\n",
    "\n",
    "loss, classification_loss, bounding_box_loss, classification_accuracy, bounding_box_mse = model.evaluate(validation_dataset, steps=1)\n",
    "print(\"Validation accuracy: \", classification_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz-b8TxU6EDj"
   },
   "outputs": [],
   "source": [
    "plot_metrics(\"classification_loss\", \"Classification Loss\")\n",
    "plot_metrics(\"bounding_box_loss\", \"Bounding Box Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FBn4V5-Krkt"
   },
   "source": [
    "## Intersection over union\n",
    "\n",
    "Calculate the I-O-U metric to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFqJxt3_VrCm"
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(pred_box, true_box):\n",
    "    xmin_pred, ymin_pred, xmax_pred, ymax_pred =  np.split(pred_box, 4, axis = 1)\n",
    "    xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, 4, axis = 1)\n",
    "\n",
    "    smoothing_factor = 1e-10\n",
    "\n",
    "    xmin_overlap = np.maximum(xmin_pred, xmin_true)\n",
    "    xmax_overlap = np.minimum(xmax_pred, xmax_true)\n",
    "    ymin_overlap = np.maximum(ymin_pred, ymin_true)\n",
    "    ymax_overlap = np.minimum(ymax_pred, ymax_true)\n",
    "\n",
    "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
    "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
    "\n",
    "    overlap_area = np.maximum((xmax_overlap - xmin_overlap), 0)  * np.maximum((ymax_overlap - ymin_overlap), 0)\n",
    "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
    "    \n",
    "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jFVovcUUVs1"
   },
   "source": [
    "### Visualize predictions\n",
    "The following code will make predictions and visualize both the classification and the predicted bounding boxes.\n",
    "- The true bounding box labels will be in green, and the model's predicted bounding boxes are in red.\n",
    "- The predicted number is shown below the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w12OId8Mz7dF"
   },
   "outputs": [],
   "source": [
    "# recognize validation digits\n",
    "predictions = model.predict(validation_digits, batch_size=64)\n",
    "predicted_labels = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "predicted_bboxes = predictions[1]\n",
    "\n",
    "iou = intersection_over_union(predicted_bboxes, validation_bboxes)\n",
    "\n",
    "iou_threshold = 0.6\n",
    "\n",
    "print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\n",
    "print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))\n",
    "\n",
    "\n",
    "display_digits_with_boxes(validation_digits, predicted_labels, validation_labels, predicted_bboxes, validation_bboxes, iou, \"True and Predicted values\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W1_Lab_3_Object_Localization.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
